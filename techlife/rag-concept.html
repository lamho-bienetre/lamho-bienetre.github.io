

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/hola_favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Lam Ho">
  <meta name="keywords" content="">
  
    <meta name="description" content="搭建 RAG 要知道的都在这里了。">
<meta property="og:type" content="article">
<meta property="og:title" content="RAG架构，源于AI，不止AI">
<meta property="og:url" content="https://lamho-bienetre.github.io/techlife/rag-concept.html">
<meta property="og:site_name" content="Always remember me this way">
<meta property="og:description" content="搭建 RAG 要知道的都在这里了。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://lamho-bienetre.github.io/img/wallhaven-6qmwww.jpg">
<meta property="article:published_time" content="2025-07-05T12:00:00.000Z">
<meta property="article:modified_time" content="2025-07-06T17:11:15.413Z">
<meta property="article:author" content="Lam Ho">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://lamho-bienetre.github.io/img/wallhaven-6qmwww.jpg">
  
  
  
  <title>RAG架构，源于AI，不止AI - Always remember me this way</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"lamho-bienetre.github.io","root":"/","version":"1.9.4","typing":{"enable":false,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":6},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Always remember me this way</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/wallhaven-6qmwww.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">RAG架构，源于AI，不止AI</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-07-05 20:00" pubdate>
          2025-07-05
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          42 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">RAG架构，源于AI，不止AI</h1>
            
            
              <div class="markdown-body">
                
                <blockquote style="font-size: 14px;color: White; background-color: darkslategray; padding: 15px; border-radius: 10px; outline: 2px dotted darkslategray; outline-offset: 5px;">
    <b style="font-size: 16px;">TL;DR</b>
    <ul style="margin: 10px 0; padding-left: 1.5em;">
        <li>RAG 要点</li>
    </ul>
</blockquote>

<br>
<div style="text-align: center; font-size: 16px; color: gray;">
    · · · · · ✦ ✧ ✦ · · · · ·
</div>
<br>


<p>如前文《<a href="/techlife/llm-application-basic.html" title="LLM应用，不需要学AI，会调接口就行">LLM应用，不需要学AI，会调接口就行</a>》所说，AI 对话产品中的“联网搜索”是通过检索来优化 LLM 的生成结果，这种技术架构称为 <strong>RAG（Retrieval-Augmented Generation，检索增强生成）</strong>。现在 RAG 在业界十分常见，用于搭建基于私有知识库的问答场景。通过检索从企业或个人的私有知识库中提取相关信息，结合 LLM 进行生成，有效保证问答内容的专业性和私密性。</p>
<h1 id="1-Retrieve（检索）"><a href="#1-Retrieve（检索）" class="headerlink" title="1. Retrieve（检索）"></a>1. Retrieve（检索）</h1><p>在 RAG 架构中，检索环节是核心步骤，决定了生成内容的相关性和准确性。LLM 实现“联网搜索”，数据源就是互联网数据，检索方式就是搜索引擎，直接调用搜索引擎 API 就可以解决了。那么我们的数据源换成私有知识库，检索又如何实现呢？</p>
<h2 id="1-1-基本的检索实现"><a href="#1-1-基本的检索实现" class="headerlink" title="1.1. 基本的检索实现"></a>1.1. 基本的检索实现</h2><h3 id="检索策略"><a href="#检索策略" class="headerlink" title="检索策略"></a>检索策略</h3><p>当前很常用的检索策略是 <font style="color:orange;"><strong>Hybrid search（混合搜索）</strong></font>。Hybrid search 是指 <u>Lexical search（词汇搜索）</u>和 <u>Semantic search（语义搜索）</u>两种搜索类别的结合。</p>
<p><font style="color:orange;"><strong>Lexical search（词汇搜索）</strong></font>，主要技术有 <font style="color:violet;">Keyword search（关键词搜索）</font>和 <font style="color:violet;">Full-text search（全文搜索）</font>。Keyword search（关键词搜索），顾名思义是找到精准匹配的关键词。而 Full-text search（全文搜索）可以看作是在关键词匹配的基础上，引入了部分 NLP 技术，从而支持模糊匹配和更灵活的检索。</p>
<p>词汇搜索的应用很常见：</p>
<ul>
<li>电商平台搜索产品（例如在淘宝搜扫地机器人可以找到相关的商品）</li>
<li>博客或知识库全文中搜索明确关键词（例如在本站搜 RAG 可以找到提到这个词语的文章）</li>
<li>图书馆系统里搜索书名</li>
</ul>
<p>显然词汇搜索有一定的局限性，它<i style="color:tomato;">无法找到同义词、无法理解上下文、拼写错误会影响搜索结果</i>。而 Semantic search（语义搜索）可以补足词汇搜索的这些缺点。</p>
<p><font style="color:orange;"><strong>Semantic search（语义搜索）</strong></font>，在当前 RAG 架构主要就是使用 <font style="color:violet;">Vector search（向量搜索）</font>。</p>
<p>把文本转化成向量，也就是 embedding（文本向量化），向量里包含了语义信息。通过计算两个文本的向量距离（相似度），就可以判断它们是否相似。我们只需要知道，把文本用向量来表示是包含语义的，做向量检索就可以匹配到语义相近的文本。各厂商有提供 embedding 模型的接口，直接调用即可。比如调用 <a target="_blank" rel="noopener" href="https://platform.openai.com/docs/guides/embeddings/embedding-models">OpenAI - Embeddings</a>，传入一个文本可以得到一个1536维度的数组。</p>
<p>假设我们的知识库是所有 helpdesk 工单，每个单的文本做 embedding 变成向量存储到数据库里。新增了一个新的工单，我们想要搜到和新工单语义最接近的 5 个单，那就把新工单做 embedding 变成一个向量，计算这个向量和数据库里所有向量的距离，相似度最高的 5 个就是我们要找的单。</p>
<p><img src="/img/2025-07-05-rag-basic/helpdesk_vector_search.png" srcset="/img/loading.gif" lazyload></p>
<p>注意，请求搜索的文本做 embedding 时选用的模型，必须与数据库中向量化所使用的模型一致。因为不同模型生成的向量完全不同，不同 embedding 模型之间不能通用。如果要更换 embedding 模型，需要把数据库中已存储的数据重新做 embedding 写入。</p>
<p>语义搜索看起来好像比词汇搜索更高级，但也存在不足，它<i style="color:tomato;">不能识别新词和低频词</i>，而这些用词汇搜索是可以实现的。两者互补，使得“词汇搜索+语义搜索”这种既要又要的混合搜索方式成为 RAG 技术栈中的主流。</p>
<blockquote style="padding: 15px; border-radius: 10px; border-left: none; outline: 3px dotted darkslategray;">
    <b style="color: orange;">Hybrid search = Lexical search + Semantic search</b>
    <ul style="margin: 10px 0; padding-left: 1.5em;">
        <li><b>Lexical search</b>（词汇搜索，如关键词搜索、全文搜索、……）
            <ul>
                <li>优：匹配精准、支持新词/罕见词、能识别拼写精确的查询</li>
                <li>劣：缺乏语义理解、无法识别同义词、对拼写错误敏感</li>
            </ul>
        </li>
        <li><b>Semantic search</b>（语义搜索，如向量搜索、……）
            <ul>
                <li>优：理解上下文含义、支持模糊匹配、能识别表达差异的相似内容</li>
                <li>劣：无法识别新词/低频词、对精确查询不敏感、依赖模型一致性</li>
            </ul>
        </li>
    </ul>
</blockquote>



<h3 id="Vector-database（向量数据库）"><a href="#Vector-database（向量数据库）" class="headerlink" title="Vector database（向量数据库）"></a>Vector database（向量数据库）</h3><p>要实现以上所说的检索策略，Vector database（向量数据库）就进入了大众视野。现在有很多向量数据库可选，常见的有：Chroma &#x2F; FAISS &#x2F; Qdrant &#x2F; Milvus &#x2F; Pinecone &#x2F; Weaviate &#x2F; Elasticsearch &#x2F; PostgreSQL（+ pgvector插件）等等，还有 Azure 的 AI Search 服务和 Cosmos DB。这些向量数据库都支持混合搜索，也可以选择只用词汇搜索或者只用向量搜索。使用这些向量数据库，无需自己写搜索算法，无需搞清楚两类搜索是怎么混合的，依照各自的文档实现即可。当然在混合搜索里也可以调整一些参数，比如两类搜索的权重，这就需要进一步了解混合搜索实现的细节了。</p>
<h2 id="1-2-检索的进阶优化"><a href="#1-2-检索的进阶优化" class="headerlink" title="1.2. 检索的进阶优化"></a>1.2. 检索的进阶优化</h2><p>即使使用了混合搜索，但效果可能依然不佳。为了解决词汇搜索和语义搜索的局限性，在进行混合搜索后，又有对检索结果进行二次处理的策略。</p>
<h3 id="1-2-1-用-Reranking"><a href="#1-2-1-用-Reranking" class="headerlink" title="1.2.1. 用 Reranking"></a>1.2.1. 用 Reranking</h3><p>在 RAG 实现里 reranking 是个高频词。 Reranking，看字面意思就是重排，在进行初步检索后，训练一个模型来评估请求搜索的文本与每个候选项的匹配度，从而二次排序，得到更准确的结果。</p>
<p><strong>那做 reranking 和做 embedding 算相似度有什么不同呢？</strong></p>
<blockquote>
<p>按我的理解，我会把 <u><b>reranking</b> VS <b>vector search</b></u> 类比成 <u><b>因果推断</b> VS <b>相关性</b></u>。如果你没有统计学的背景，那么我举一个例子说明。</p>
<p><i>比如，夏天人们会吃更多的冰淇淋，也更容易晒伤。这两件事之间确实存在相关性，但吃冰淇淋并不会导致晒伤，晒伤也不会反过来让人去吃冰淇淋。它们的背后其实是同一个原因：天气炎热。</i></p>
<p>这就好比 vector search 只能找到“看起来相关”的内容，而 reranking 进一步判断“这些相关里，哪些是和用户的问题真正相关的内容”。</p>
</blockquote>
<p><strong>reranking 是如何改善检索效果呢？</strong></p>
<blockquote>
<p>（以下为虚拟案例说明 reranking 机制，并非使用模型就能达到这种效果。）<br>比如用户查询：<font style="color: Violet;">“法律援助如何申请”</font></p>
<p>初步检索返回的 Top-5 结果：</p>
<table>
<thead>
<tr>
<th>rank</th>
<th>text</th>
<th>remark</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>「中国法律体系包括民法、刑法、行政法等，是国家治理的基石之一。」</td>
<td>（语义有“法律”，但和“申请援助”无关）</td>
</tr>
<tr>
<td>2</td>
<td>「很多人不知道，他们在遇到法律问题时可以申请法律援助，这通常适用于收入较低的人群。」</td>
<td>（提到了“可以申请”，但没说如何申请）</td>
</tr>
<tr>
<td>3</td>
<td>「申请法律援助的第一步是前往当地司法局，填写申请表，并提交收入证明和案件材料。」</td>
<td>✅ （明确回答“如何申请”）</td>
</tr>
<tr>
<td>4</td>
<td>「在一些地区，法律援助可以通过线上平台提交申请，节省了大量时间。」</td>
<td>✅ （也在讲申请流程）</td>
</tr>
<tr>
<td>5</td>
<td>「法律援助制度的发展源于对社会公平正义的追求，其在许多国家已逐步制度化。」</td>
<td>（背景知识，有些远）</td>
</tr>
</tbody></table>
<p>reranking 后的结果（结合用户问题精准排序）：</p>
<table>
<thead>
<tr>
<th>rerank</th>
<th>text</th>
<th>remark</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>「申请法律援助的第一步是前往当地司法局，填写申请表，并提交收入证明和案件材料。」</td>
<td>✅ （明确回答“如何申请”）</td>
</tr>
<tr>
<td>2</td>
<td>「在一些地区，法律援助可以通过线上平台提交申请，节省了大量时间。」</td>
<td>✅ （也在讲申请流程）</td>
</tr>
<tr>
<td>3</td>
<td>「很多人不知道，他们在遇到法律问题时可以申请法律援助，这通常适用于收入较低的人群。」</td>
<td>（提到了“可以申请”，但没说如何申请）</td>
</tr>
<tr>
<td>4</td>
<td>「法律援助制度的发展源于对社会公平正义的追求，其在许多国家已逐步制度化。」</td>
<td>（背景知识，有些远）</td>
</tr>
<tr>
<td>5</td>
<td>「中国法律体系包括民法、刑法、行政法等，是国家治理的基石之一。」</td>
<td>（语义有“法律”，但和“申请援助”无关）</td>
</tr>
</tbody></table>
</blockquote>
<p>reranking 用来优化检索结果，那么我们需要一个很棒的 reranking 模型。reranking 的实际效果褒贬不一，甚至有人称起了反效果。reranking 需要的是更精确更强的模型，这也导致加入 reranking 会降低响应速度。常见的例子是在 Top-50~100 的检索结果里做 reranking。那如果本身数据量没有这么大，或者做完基础的检索策略取 Top-N 没到 50 就已经有不少候选项和用户问题没有什么关系了，把 reranking 加进来反而更累赘。以我当前接触到的案例，未有使用 reranking 优化检索结果的需求，且 reranking 需要消耗更多资源，未作深入研究。</p>
<h3 id="1-2-2-用-LLM"><a href="#1-2-2-用-LLM" class="headerlink" title="1.2.2. 用 LLM"></a>1.2.2. 用 LLM</h3><p>即使 embedding 模型迭代得更优秀了，初步检索的效果很好了，但还存在一个问题，就是检索结果里取 Top-N 再给 LLM 生成回复，这个 N 取多少合适呢？</p>
<p>N 取值并不是有多大取多大就行了。有人可能会认为 input token 能吃下多少就丢多少进去，这样可以尽量减少信息的丢失。尤其是与 LLM 刚火起来时对比，现在很多 LLM 可以接纳的 input token 变得非常大，很容易让人误以为传给模型的信息越多就能得到越好的效果。从实际测试的效果来看，传很长的检索结果（包含一些内容不太相关的候选项）让 LLM 根据这一大堆检索结果来回答用户的提问，会降低回复的质量。有论文证实，LLM 在处理长文本时，对开头&#x2F;结尾的信息理解表现比较好，而位于中间的信息会被忽略。<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)">[1]</span></a></sup></p>
<p>在了解 reranking 之前为了解决这个问题，我尝试过把检索结果传给 LLM 指令它筛出和问题相关的候选项，再做最后一步的生成回复，实际效果还不错。当然这个策略也是会降低响应速度。在网上也有看到有人放弃了 reranking，转而用 LLM 来优化检索结果。目前来看，如果需要优化检索结果，我会选择多加一步 inference LLM 来实现。</p>
<h1 id="2-Chunking（切分）"><a href="#2-Chunking（切分）" class="headerlink" title="2. Chunking（切分）"></a>2. Chunking（切分）</h1><p>Chunking，指的是长文本切分成小的 chunk。当文本太长，超出 embedding 和 LLM 的 input token 限制的时候就必须要做切分了。或者是基于产品的设计，比如想要展示引用的文本来源精确到句子&#x2F;段落，那么即使文本不长，也需要做切分。</p>
<ul>
<li><p>是否需要切分，需要具体场景具体分析。</p>
</li>
<li><p>如何切分，没有一个万能的标准策略，需要根据文档结构、产品设计和业务目标等方方面面来制定切分策略。</p>
</li>
</ul>
<p>比如 LLM 很常用的工具 LangChain，它的 <a target="_blank" rel="noopener" href="https://python.langchain.com/docs/concepts/text_splitters/">Text splitters</a> 模块提供了多种切分策略，包括基于长度的切分（按文本长度或 token 长度）、基于文本结构的切分（如段落、句子、单词）、基于文档结构的切分（如 HTML、Markdown、JSON、代码等）、以及基于语义的切分（通过 embedding 判断语义变化点）。另外，还可以设置前后两个 chunk 之间的文本重叠数量（<code>chunk_overlap</code>），解决切分后上下文断裂的问题，提高后续的检索效果。</p>
<p>还有针对不同领域的切分策略，比如财务报表、法律文件等。基于这些特定领域的文件特性，又衍生出各种各样的策略。可见切分策略这一块要下的功夫还是挺多的。</p>
<h1 id="3-实现-RAG-需要考虑的"><a href="#3-实现-RAG-需要考虑的" class="headerlink" title="3. 实现 RAG 需要考虑的"></a>3. 实现 RAG 需要考虑的</h1><p><img src="/img/2025-07-05-rag-basic/rag_workflow.png" srcset="/img/loading.gif" lazyload><font style="font-size: 12px;">Source: <a target="_blank" rel="noopener" href="https://wandb.ai/cosmo3769/RAG/reports/A-Gentle-Introduction-to-Retrieval-Augmented-Generation-RAG---Vmlldzo1MjM4Mjk1#rag-components-">A Gentle Introduction to Retrieval Augmented Generation (RAG)</a></font></p>
<p>从以上所说的 RAG 架构来看，主要包含：检索、切分以及底层的 LLM。</p>
<h2 id="3-1-关于-Retrieve-检索需要考虑的"><a href="#3-1-关于-Retrieve-检索需要考虑的" class="headerlink" title="3.1 关于 Retrieve 检索需要考虑的"></a>3.1 关于 Retrieve 检索需要考虑的</h2><ol>
<li><p><strong>检索策略的选择</strong><br> 理论上混合搜索是最佳选择，但实际应用中有可能只用词汇搜索或者只用向量搜索就可以达到类似的效果，使用单一的检索策略可以节省资源。使用不同的策略对检索效果的差异取决于具体任务的数据特性和查询需求。如果要评估哪种模式效果更好，需要人工在上线应用后检查检索结果这个中间步骤。我会直接闭眼选择混合搜索。</p>
</li>
<li><p><strong>embedding 模型的选择</strong><br> embedding 包含语义信息，embedding 模型的好坏会影响检索结果。我会选择当前最新且价格不高的模型，选定 embedding 模型后不会轻易更换，因为如果更换需要重新对数据库已存储的数据重新读写存储。作为 AI 应用方，embedding 模型的优化不是我们努力的地方。</p>
</li>
<li><p><strong>检索结果的 Top-N 筛选</strong><br> N 选大一点取到不相关的候选项，再用 LLM 来筛选出强相关的项进行优化。具体数值要基于实际案例的测试效果。</p>
</li>
<li><p><strong>向量数据库的选择</strong><br> 常见的向量数据库都有那么多种选择，可见没有哪一个是完全优于其他的。如何选择适合自己场景的向量数据库，需要基于具体任务对性能、价格和数据安全等多方面进行探索。</p>
</li>
</ol>
<h2 id="3-2-关于-Chunking-切分需要考虑的"><a href="#3-2-关于-Chunking-切分需要考虑的" class="headerlink" title="3.2 关于 Chunking 切分需要考虑的"></a>3.2 关于 Chunking 切分需要考虑的</h2><p>切分策略的搭</p>
<hr>
<span style="color: Gray;">

<section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.03172">Lost in the Middle: How Language Models Use Long Contexts</a>
<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Tech-Life/" class="category-chain-item">Tech &amp; Life</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>RAG架构，源于AI，不止AI</div>
      <div>https://lamho-bienetre.github.io/techlife/rag-concept.html</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Lam Ho</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>July 5, 2025</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/techlife/llm-application-basic.html" title="LLM应用，不需要学AI，会调接口就行">
                        <span class="hidden-mobile">LLM应用，不需要学AI，会调接口就行</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
