<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2025年度回顾 / 写作</title>
    <link href="/moments/2025-review-writing.html"/>
    <url>/moments/2025-review-writing.html</url>
    
    <content type="html"><![CDATA[<p>很多年前就想要写点东西，但一直没有行动。年头因为备受家庭压力在线找朋友吐槽，倒是因此有了新发现。</p><p>我发现在编辑消息的过程中，就是在梳理事情的始末和自己的感受。消息发出去之后朋友还没有回复我，但是我的郁结好像已经有了一定程度的缓解。</p><p>把事情写出来，自己再去阅读，仿佛是从他人角度去看这件事。把自己抽离出来，就没这么痛苦了。所谓旁观者清，以第三视角来挖掘自己内心深处的诉求，把一堆混沌的毛线慢慢理好。一时理不清也没关系，能理一点是一点。下次想起来再理，或者忘掉了也是一桩好事。</p><p>我选择了在之前搭建的 blog 网站下笔。这个网站本来是为写技术 blog 搭的，但搭好之后没有写过一篇文章。写了文章之后发给了几个朋友看。今天，blog 已经消失得无影无踪，朋友看到我开始写 blog 都很感叹，很久没有看到朋友分享长文了。</p><p>我没有给写作范围设限，想写什么就写什么。在自媒体泛滥的今天，人人都想着“垂直”。垂直，指的是专精一个细分领域。基于社交平台的推荐机制，你的输出越垂直，起号越快，触达的用户粘性越高。如今不少人开社媒账号是一心想着起号赚钱。我很清楚我输出的目的并非如此。</p><p>认真开始写东西才发现自己确实没有写作天赋。经常会觉得词不达意，但是我又实在想不到更好的表达。不过我倒觉得这个不是个大问题，持续写下去应该会有所改善。写到 50 岁，写到 60 岁，坚持写下去，写到老花眼，写到头发花白，写作这条路可以走到生命的最后。</p><br><center><img src="/img/2026-01-04-2025-review-writing/blog_categories_count.jpg" style="width:70%;"></center><p>回顾一下去年的产出，没想到 Tech &amp; Life 这个科技栏目的产量排首位！我当初搭这个技术 blog 一直没有产出，就是觉得实在不想写 tech，我感觉并不擅长我的工作，写 tech 好难。现在看来，工作还是占据我生活的很大一部分，我的精力大部分都放在了工作上。主要也是我找到了我想写的 tech 方向，更偏向商业应用和理论，而不是写搭建项目的教程。</p><p>2026 年的写作计划，会继续保持 tech 的输出，同时其他栏目也要跟上，想办法提高生产力！其实在其他栏目现存也有几篇草稿，都是写到一半就不管了。光开坑不填坑这个不良习惯要改改。</p>]]></content>
    
    
    <categories>
      
      <category>Moments</category>
      
    </categories>
    
    
    <tags>
      
      <tag>2025 Review</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>又到圣诞，又想喝热红酒了</title>
    <link href="/moments/joyeux-noel.html"/>
    <url>/moments/joyeux-noel.html</url>
    
    <content type="html"><![CDATA[<p>🎵 又到圣诞 &#x2F; 又到圣诞 &#x2F; 上个圣诞冇今年咁叹<br>原词是“下个圣诞有冇今年咁叹”（创作背景懂的都懂，不懂的随便啦），是对未来不确定性的隐忧。而我的信念是要一年比一年好，活在当下，向前看。</p><p>圣诞到了，我又想喝热红酒了。</p><p>想起第一次逛圣诞集市，我就觉得和广东春节的花街很相似。在广场上会搭建一排排的小木屋，每一间木屋就是一个摊位，有的卖饰品，有的卖食物。圣诞集市大概在圣诞节前一个月就开始有了，有空有心情的话一年可以逛很多个圣诞集市。晚上配上灯饰的圣诞集市更有氛围感。</p><p>国内各大商场在圣诞节前后也会布置圣诞树和各种圣诞元素的装饰。不过没有熙熙攘攘的圣诞集市，感觉还是很不一样的。</p><p>每年快到圣诞的时候就很想喝欧洲圣诞市集卖的热红酒。热红酒是用红酒加上香料煮制，温热带点甜。</p><p>试过在广州的酒馆点热红酒，售价和鸡尾酒一样，用玻璃酒杯装着，杯边还有小装饰。这种精致版热红酒，和我记忆中的味道相去甚远。</p><p>市集上售卖的热红酒是煮好装在一个大锅里，摊主舀出一杯一杯卖给顾客，顾客拿着边逛边喝。有的杯子是一次性杯，喝完就扔了；有的杯子是带有图案的纪念杯，喜欢收集就可以带回家，退还给摊主就可以拿回杯子的押金。</p><p>我又试过自己在家煮热红酒，其实烹调方式很简单。但是家里只有我喝，煮一锅自己喝不完。</p><p>我思念的或许不是热红酒，而是过去的时光。在冬日的寒夜里，和朋友逛圣诞集市，买一杯热红酒捧在手里喝，那种暖洋洋的感觉。</p>]]></content>
    
    
    <categories>
      
      <category>Moments</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>大模型的前世今生</title>
    <link href="/techlife/llm-history-intro.html"/>
    <url>/techlife/llm-history-intro.html</url>
    
    <content type="html"><![CDATA[<blockquote style="font-size: 14px;color: White; background-color: darkslategray; padding: 15px; border-radius: 10px; outline: 2px dotted darkslategray; outline-offset: 5px;">    <b style="font-size: 16px;">TL;DR</b>    <ul style="margin: 10px 0; padding-left: 1.5em;">    <li>从人工神经网络的最小粒度感知器说起，简单例子演示模型的参数从哪来。</li>    <li>解构如何从一个神经元发展到复杂的人工神经网络，模型参数如何一路膨胀。</li>    </ul></blockquote><br><div style="text-align: center; font-size: 16px; color: gray;">· · · · · ✦ ✧ ✦ · · · · ·</div><br><center><img src="/img/2025-12-01-llm-history-intro/llama_2_params.png" style="width:70%;"><span style="display:block; font-size:12px; color:gray; text-align:left;  word-break:break-all;">https://blogs.microsoft.com/blog/2023/07/18/microsoft-and-meta-expand-their-ai-partnership-with-llama-2-on-azure-and-windows/</span></center><br><p>在 Llama 2 发布的时候，我们看到它提供了 3 种不同的 <em>参数</em> 量级：7B，13B，70B。这个 B 是什么意思呢？</p><blockquote style="padding: 15px; border-radius: 10px; border-left: none; outline: 3px dotted darkslategray;">B 表示的是 billion，1B=1000,000,000，也就是 10 亿。所谓大模型，就是“大”在这几十亿的参数量上。</blockquote><p>不难留意到，闭源模型的参数量是不公开的，比如 GPT-4o、Claude 3 Opus、Gemini 1.5 Ultra 等等，无论在模型命名还是官方文档上我们都找不到具体参数量的描述。参数量在闭源模型里更多是一种商业机密，不公开可以避免过度横向比较和技术拆解。</p><p>但在 open-weight 模型里，也就是我们俗称的“开源”模型，我们都能查到具体的参数量，像 Llama 就把参数量直接写在了模型名称上。</p><p><span style="color: gray; font-size:12px;"><span style="color: white; background-color: violet; padding: 0 2px;"><b>小注</b></span> 想了解 open-weight 和 open-source 有什么不同，可以看这篇《<a href="/techlife/llm-open-source-vs-open-weight.html" title="揭开大模型的假开源遮羞布">揭开大模型的假开源遮羞布</a>》。</span></p><p>下载 open-weight 模型最常用的平台 Hugging Face，在平台的模型选择页面，有一个筛选条件 Parameters。可见参数量的大小是用户选择模型的一个很重要因素。</p><center><img src="/img/2025-12-01-llm-history-intro/hugging_face_params.jpg" style="width:80%;"></center><br><p>Open-weight 模型公开参数量，是为了让用户能评估所需要的计算资源（如 GPU 显存），并且方便拿来相互比较和共同改进。</p><p>为什么大模型的参数可以大到几十亿甚至上百亿，我们从 perceptron（感知器）—— 人工神经网络的雏形讲起。</p><h1 id="1-Perceptron-感知器"><a href="#1-Perceptron-感知器" class="headerlink" title="1. Perceptron 感知器"></a>1. Perceptron 感知器</h1><p><span style="color:tomato;"><strong>感知器（Perceptron）</strong></span>由美国心理学家 Frank Rosenblatt 在 1957 年提出，是为了模拟人类神经元的工作方式，可以解决简单的二分类问题。最早用于图像识别任务，比如 Rosenblatt 所设计的用于训练感知器的点阵卡片（如下图）。你可以想象点阵卡片上的黑白格子排列成 X 字母的形状，感知器的目标是判断点阵图案是否是 X。</p><center><img src="/img/2025-12-01-llm-history-intro/rosenblatt.jpg" style="width:100%;"><span style="display:block; font-size:12px; color:gray; text-align:left;  word-break:break-all;">https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon</span></center><br><p>感知器是如何模拟人类神经元的工作方式呢？它模拟神经元接收突触信号，当累加的输入信号达到阈值时触发输出，通过这种<span style="color:orange;"><strong>“加权累加 + 阈值决策”</strong></span>的机制来实现。</p><div style="text-align:center;"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -5.665ex;" xmlns="http://www.w3.org/2000/svg" width="32.141ex" height="12.46ex" role="img" focusable="false" viewBox="0 -3003.7 14206.1 5507.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-LO-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-1-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path><path id="MJX-1-TEX-S3-7B" d="M618 -943L612 -949H582L568 -943Q472 -903 411 -841T332 -703Q327 -682 327 -653T325 -350Q324 -28 323 -18Q317 24 301 61T264 124T221 171T179 205T147 225T132 234Q130 238 130 250Q130 255 130 258T131 264T132 267T134 269T139 272T144 275Q207 308 256 367Q310 436 323 519Q324 529 325 851Q326 1124 326 1154T332 1205Q369 1358 566 1443L582 1450H612L618 1444V1429Q618 1413 616 1411L608 1406Q599 1402 585 1393T552 1372T515 1343T479 1305T449 1257T429 1200Q425 1180 425 1152T423 851Q422 579 422 549T416 498Q407 459 388 424T346 364T297 318T250 284T214 264T197 254L188 251L205 242Q290 200 345 138T416 3Q421 -18 421 -48T423 -349Q423 -397 423 -472Q424 -677 428 -694Q429 -697 429 -699Q434 -722 443 -743T465 -782T491 -816T519 -845T548 -868T574 -886T595 -899T610 -908L616 -910Q618 -912 618 -928V-943Z"></path><path id="MJX-1-TEX-N-69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z"></path><path id="MJX-1-TEX-N-66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z"></path><path id="MJX-1-TEX-N-A0" d=""></path><path id="MJX-1-TEX-N-3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-1-TEX-N-2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0,1441.2)"><g data-mml-node="mtd" transform="translate(25,0)"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g></g><g data-mml-node="mtd" transform="translate(490,0)"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(277.8,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="munderover" transform="translate(1333.6,0)"><g data-mml-node="mo"><use data-c="2211" xlink:href="#MJX-1-TEX-LO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(148.2,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1123,0)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="TeXAtom" transform="translate(509.9,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-1-TEX-I-1D45B"></use></g></g></g><g data-mml-node="msub" transform="translate(2944.2,0)"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g></g><g data-mml-node="msub" transform="translate(3987.2,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(5108.3,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mi" transform="translate(6108.6,0)"><use data-c="1D44F" xlink:href="#MJX-1-TEX-I-1D44F"></use></g><g data-mml-node="mstyle" transform="translate(6537.6,0)"><g data-mml-node="mspace"></g></g><g data-mml-node="mtext" transform="translate(7537.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">（</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">加</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">权</text><text data-variant="normal" transform="translate(3000,0) scale(1,-1)" font-size="884px" font-family="serif">累</text><text data-variant="normal" transform="translate(4000,0) scale(1,-1)" font-size="884px" font-family="serif">加</text><text data-variant="normal" transform="translate(5000,0) scale(1,-1)" font-size="884px" font-family="serif">）</text></g></g></g><g data-mml-node="mtr" transform="translate(0,-1554.2)"><g data-mml-node="mtd"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-1-TEX-I-1D466"></use></g><g data-mml-node="mo" transform="translate(300.6,16) translate(-250 0)"><use data-c="5E" xlink:href="#MJX-1-TEX-N-5E"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(490,0)"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(277.8,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mrow" transform="translate(1333.6,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><use data-c="7B" xlink:href="#MJX-1-TEX-S3-7B"></use></g><g data-mml-node="mtable" transform="translate(750,0)"><g data-mml-node="mtr" transform="translate(0,600)"><g data-mml-node="mtd"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mtd" transform="translate(1500,0)"><g data-mml-node="mtext"><use data-c="69" xlink:href="#MJX-1-TEX-N-69"></use><use data-c="66" xlink:href="#MJX-1-TEX-N-66" transform="translate(278,0)"></use><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0" transform="translate(584,0)"></use></g><g data-mml-node="mi" transform="translate(834,0)"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="mo" transform="translate(1576.8,0)"><use data-c="3E" xlink:href="#MJX-1-TEX-N-3E"></use></g><g data-mml-node="mn" transform="translate(2632.6,0)"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0,-600)"><g data-mml-node="mtd"><g data-mml-node="mn"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g></g><g data-mml-node="mtd" transform="translate(1500,0)"><g data-mml-node="mtext"><use data-c="69" xlink:href="#MJX-1-TEX-N-69"></use><use data-c="66" xlink:href="#MJX-1-TEX-N-66" transform="translate(278,0)"></use><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0" transform="translate(584,0)"></use></g><g data-mml-node="mi" transform="translate(834,0)"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="mo" transform="translate(1576.8,0)"><use data-c="2264" xlink:href="#MJX-1-TEX-N-2264"></use></g><g data-mml-node="mn" transform="translate(2632.6,0)"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g></g></g></g><g data-mml-node="mo" transform="translate(5382.6,0) translate(0 250)"></g></g><g data-mml-node="mstyle" transform="translate(6716.1,0)"><g data-mml-node="mspace"></g></g><g data-mml-node="mtext" transform="translate(7716.1,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">（</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">阈</text><text data-variant="normal" transform="translate(2000,0) scale(1,-1)" font-size="884px" font-family="serif">值</text><text data-variant="normal" transform="translate(3000,0) scale(1,-1)" font-size="884px" font-family="serif">决</text><text data-variant="normal" transform="translate(4000,0) scale(1,-1)" font-size="884px" font-family="serif">策</text><text data-variant="normal" transform="translate(5000,0) scale(1,-1)" font-size="884px" font-family="serif">）</text></g></g></g></g></g></g></svg></mjx-container></div><br><p>举个例子，假设我们要做一个决策任务决定“是否出门散步”，有 3 个因素要考虑，例如以下 3 个特征：</p><div style="text-align: center;">    <span style="font-size: 12px; display: inline-block;">        <table style="margin: 0 auto; border-collapse: collapse; text-align: left;">          <thead>            <tr>              <th><b>特征</b></th>              <th><b>取值</b></th>              <th><b>决策相关度</b></th>            </tr>          </thead>          <tbody>            <tr>              <td><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.361ex" height="1.344ex" role="img" focusable="false" viewBox="0 -444 1043.6 594" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-B-1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D431" xlink:href="#MJX-1-TEX-B-1D431"></use></g></g><g data-mml-node="mn" transform="translate(640,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g></g></g></svg></mjx-container>：天气好吗</td>              <td>1 (是) / 0 (否)</td>              <td>高度正相关</td>            </tr>            <tr>              <td><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.361ex" height="1.344ex" role="img" focusable="false" viewBox="0 -444 1043.6 594" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-B-1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D431" xlink:href="#MJX-1-TEX-B-1D431"></use></g></g><g data-mml-node="mn" transform="translate(640,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g></g></g></svg></mjx-container>：我有空吗</td>              <td>1 (是) / 0 (否)</td>              <td>中度正相关</td>            </tr>            <tr>              <td><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="2.361ex" height="1.379ex" role="img" focusable="false" viewBox="0 -444 1043.6 609.6" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-B-1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path><path id="MJX-1-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D431" xlink:href="#MJX-1-TEX-B-1D431"></use></g></g><g data-mml-node="mn" transform="translate(640,-150) scale(0.707)"><use data-c="33" xlink:href="#MJX-1-TEX-N-33"></use></g></g></g></g></svg></mjx-container>：我累吗</td>              <td>1 (是) / 0 (否)</td>              <td>高度负相关</td>            </tr>          </tbody>        </table>    </span></div><br><div style="text-align:center;">    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -3.619ex;" xmlns="http://www.w3.org/2000/svg" width="30.444ex" height="8.369ex" role="img" focusable="false" viewBox="0 -2099.5 13456.1 3699" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-1-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-1-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path><path id="MJX-1-TEX-S3-7B" d="M618 -943L612 -949H582L568 -943Q472 -903 411 -841T332 -703Q327 -682 327 -653T325 -350Q324 -28 323 -18Q317 24 301 61T264 124T221 171T179 205T147 225T132 234Q130 238 130 250Q130 255 130 258T131 264T132 267T134 269T139 272T144 275Q207 308 256 367Q310 436 323 519Q324 529 325 851Q326 1124 326 1154T332 1205Q369 1358 566 1443L582 1450H612L618 1444V1429Q618 1413 616 1411L608 1406Q599 1402 585 1393T552 1372T515 1343T479 1305T449 1257T429 1200Q425 1180 425 1152T423 851Q422 579 422 549T416 498Q407 459 388 424T346 364T297 318T250 284T214 264T197 254L188 251L205 242Q290 200 345 138T416 3Q421 -18 421 -48T423 -349Q423 -397 423 -472Q424 -677 428 -694Q429 -697 429 -699Q434 -722 443 -743T465 -782T491 -816T519 -845T548 -868T574 -886T595 -899T610 -908L616 -910Q618 -912 618 -928V-943Z"></path><path id="MJX-1-TEX-N-A0" d=""></path><path id="MJX-1-TEX-N-69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z"></path><path id="MJX-1-TEX-N-66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z"></path><path id="MJX-1-TEX-N-3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-1-TEX-N-2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0,1349.5)"><g data-mml-node="mtd" transform="translate(25,0)"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g></g><g data-mml-node="mtd" transform="translate(490,0)"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(277.8,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(1333.6,0)"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="msub" transform="translate(2486.1,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(3716.9,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="msub" transform="translate(4717.1,0)"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g><g data-mml-node="msub" transform="translate(5869.7,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(7100.4,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="msub" transform="translate(8100.7,0)"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><use data-c="33" xlink:href="#MJX-1-TEX-N-33"></use></g></g><g data-mml-node="msub" transform="translate(9253.2,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><use data-c="33" xlink:href="#MJX-1-TEX-N-33"></use></g></g><g data-mml-node="mo" transform="translate(10484,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mi" transform="translate(11484.2,0)"><use data-c="1D44F" xlink:href="#MJX-1-TEX-I-1D44F"></use></g></g></g><g data-mml-node="mtr" transform="translate(0,-650)"><g data-mml-node="mtd"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-1-TEX-I-1D466"></use></g><g data-mml-node="mo" transform="translate(300.6,16) translate(-250 0)"><use data-c="5E" xlink:href="#MJX-1-TEX-N-5E"></use></g></g></g></g><g data-mml-node="mtd" transform="translate(490,0)"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(277.8,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mrow" transform="translate(1333.6,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><use data-c="7B" xlink:href="#MJX-1-TEX-S3-7B"></use></g><g data-mml-node="mtable" transform="translate(750,0)"><g data-mml-node="mtr" transform="translate(0,600)"><g data-mml-node="mtd"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mtext" transform="translate(500,0)"><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0"></use><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">（</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">出</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">门</text><text data-variant="normal" transform="translate(3250,0) scale(1,-1)" font-size="884px" font-family="serif">散</text><text data-variant="normal" transform="translate(4250,0) scale(1,-1)" font-size="884px" font-family="serif">步</text><text data-variant="normal" transform="translate(5250,0) scale(1,-1)" font-size="884px" font-family="serif">）</text></g></g><g data-mml-node="mtd" transform="translate(7750,0)"><g data-mml-node="mtext"><use data-c="69" xlink:href="#MJX-1-TEX-N-69"></use><use data-c="66" xlink:href="#MJX-1-TEX-N-66" transform="translate(278,0)"></use><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0" transform="translate(584,0)"></use></g><g data-mml-node="mi" transform="translate(834,0)"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="mo" transform="translate(1576.8,0)"><use data-c="3E" xlink:href="#MJX-1-TEX-N-3E"></use></g><g data-mml-node="mn" transform="translate(2632.6,0)"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0,-600)"><g data-mml-node="mtd"><g data-mml-node="mn"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g><g data-mml-node="mtext" transform="translate(500,0)"><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0"></use><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">（</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">宅</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">）</text></g></g><g data-mml-node="mtd" transform="translate(7750,0)"><g data-mml-node="mtext"><use data-c="69" xlink:href="#MJX-1-TEX-N-69"></use><use data-c="66" xlink:href="#MJX-1-TEX-N-66" transform="translate(278,0)"></use><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0" transform="translate(584,0)"></use></g><g data-mml-node="mi" transform="translate(834,0)"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="mo" transform="translate(1576.8,0)"><use data-c="2264" xlink:href="#MJX-1-TEX-N-2264"></use></g><g data-mml-node="mn" transform="translate(2632.6,0)"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g></g></g></g><g data-mml-node="mo" transform="translate(11632.6,0) translate(0 250)"></g></g></g></g></g></g></g></svg></mjx-container></div><br><ul><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="9.835ex" height="1.441ex" role="img" focusable="false" viewBox="0 -443 4347 637" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(1152.6,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="msub" transform="translate(1597.2,0)"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(2749.8,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="msub" transform="translate(3194.4,0)"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><use data-c="33" xlink:href="#MJX-1-TEX-N-33"></use></g></g></g></g></svg></mjx-container> 是分配给每个特征的 <span style="color:orange;"><b>权重（weights）</b></span>，表示每个特征对决策的重要程度</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D44F" xlink:href="#MJX-1-TEX-I-1D44F"></use></g></g></g></svg></mjx-container> 是 <span style="color:orange;"><b>偏置（bias）</b></span>，可以理解为模型的基础倾向或者起点</li></ul><blockquote style="padding: 15px; border-radius: 10px; border-left: none; outline: 3px dotted darkslategray;">权重（weights）和偏置（bias）一起构成了模型的 <span style="color:orange;"><b>参数（parameters）</b></span>。</blockquote><p>我们说到 open-weight 模型时，指的是公开模型的参数，也就是上面所说的权重和偏置。</p><p>如果我们取的是这组参数：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="33.056ex" height="2.009ex" role="img" focusable="false" viewBox="0 -694 14610.9 888" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-1-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(1430.3,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(2486.1,0)"><use data-c="34" xlink:href="#MJX-1-TEX-N-34"></use></g><g data-mml-node="mo" transform="translate(2986.1,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="msub" transform="translate(3430.8,0)"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(4861.1,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(5916.9,0)"><use data-c="33" xlink:href="#MJX-1-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(6416.9,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="msub" transform="translate(6861.6,0)"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mn" transform="translate(749,-150) scale(0.707)"><use data-c="33" xlink:href="#MJX-1-TEX-N-33"></use></g></g><g data-mml-node="mo" transform="translate(8291.9,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(9347.7,0)"><use data-c="2212" xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(10125.7,0)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use><use data-c="30" xlink:href="#MJX-1-TEX-N-30" transform="translate(500,0)"></use></g><g data-mml-node="mo" transform="translate(11125.7,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(11570.3,0)"><use data-c="1D44F" xlink:href="#MJX-1-TEX-I-1D44F"></use></g><g data-mml-node="mo" transform="translate(12277.1,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(13332.9,0)"><use data-c="2212" xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(14110.9,0)"><use data-c="34" xlink:href="#MJX-1-TEX-N-34"></use></g></g></g></svg></mjx-container></p><div style="text-align:center;"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="24.868ex" height="1.906ex" role="img" focusable="false" viewBox="0 -677 10991.5 842.6" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="mo" transform="translate(742.8,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1798.6,0)"><use data-c="34" xlink:href="#MJX-1-TEX-N-34"></use></g><g data-mml-node="msub" transform="translate(2298.6,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(3529.3,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(4529.6,0)"><use data-c="33" xlink:href="#MJX-1-TEX-N-33"></use></g><g data-mml-node="msub" transform="translate(5029.6,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(6260.3,0)"><use data-c="2212" xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(7260.6,0)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use><use data-c="30" xlink:href="#MJX-1-TEX-N-30" transform="translate(500,0)"></use></g><g data-mml-node="msub" transform="translate(8260.6,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><use data-c="33" xlink:href="#MJX-1-TEX-N-33"></use></g></g><g data-mml-node="mo" transform="translate(9491.3,0)"><use data-c="2212" xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(10491.5,0)"><use data-c="34" xlink:href="#MJX-1-TEX-N-34"></use></g></g></g></svg></mjx-container></div><br><div style="text-align: center;">    <span style="font-size: 12px; display: inline-block;">        <table style="margin: 0 auto; border-collapse: collapse; text-align: left;">          <thead>            <tr>              <th><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1008.6 592" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g></g></g></svg></mjx-container></th>              <th><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1008.6 592" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g></g></g></svg></mjx-container></th>              <th><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="1.375ex" role="img" focusable="false" viewBox="0 -442 1008.6 607.6" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><use data-c="33" xlink:href="#MJX-1-TEX-N-33"></use></g></g></g></g></svg></mjx-container></th>              <th><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g></g></g></svg></mjx-container></th>              <th><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.109ex" height="2.296ex" role="img" focusable="false" viewBox="0 -810 490 1015" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-1-TEX-I-1D466"></use></g><g data-mml-node="mo" transform="translate(300.6,16) translate(-250 0)"><use data-c="5E" xlink:href="#MJX-1-TEX-N-5E"></use></g></g></g></g></g></svg></mjx-container></th>              <th>描述</th>            </tr>          </thead>          <tbody>            <tr>              <td>1</td>              <td>1</td>              <td>0</td>              <td>4+3-0-4=3</td>              <td>1（出门散步）</td>              <td>天气好，有空又不累，GO！→ 这是最完美的情景❤️</td>            </tr>            <tr>              <td>1</td>              <td>1</td>              <td>1</td>              <td>4+3-10-4=-7</td>              <td>0（宅）</td>              <td>天气好又有空，但累，不出去！→ 劳累这个特征负权重设得很高，强力影响了决定结果。</td>            </tr>            <tr>              <td>0</td>              <td>1</td>              <td>0</td>              <td>0+3-0-4=-1</td>              <td>0（宅）</td>              <td>有空又不累，但天气不好，宅🏠</td>            </tr>          </tbody>        </table>    </span></div><br><p>不同的参数组合会让我们得到不一样的决策结果。假设 weights 都保持一致，我们调高一下 bias，让 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.119ex" height="1.756ex" role="img" focusable="false" viewBox="0 -694 2262.6 776" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D44F" xlink:href="#MJX-1-TEX-I-1D44F"></use></g><g data-mml-node="mo" transform="translate(706.8,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1762.6,0)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g></g></svg></mjx-container>，那么最后一种情景下，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="26.32ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 11633.2 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="mo" transform="translate(742.8,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1798.6,0)"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(2520.8,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(3521,0)"><use data-c="33" xlink:href="#MJX-1-TEX-N-33"></use></g><g data-mml-node="mo" transform="translate(4243.2,0)"><use data-c="2212" xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(5243.4,0)"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(5965.7,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(6965.9,0)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(7743.7,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(8799.4,0)"><use data-c="34" xlink:href="#MJX-1-TEX-N-34"></use></g><g data-mml-node="mo" transform="translate(9299.4,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mo" transform="translate(9688.4,0)"><use data-c="3E" xlink:href="#MJX-1-TEX-N-3E"></use></g><g data-mml-node="mn" transform="translate(10744.2,0)"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(11244.2,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></svg></mjx-container>，我们决定出门散步！</p><center><img src="/img/2025-12-01-llm-history-intro/neuron.jpg" style="width:80%;"><span  style="display:block; font-size:12px; color:gray; text-align:left;  word-break:break-all;">https://askabiologist.asu.edu/chinese-simplified/%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E8%A7%A3%E5%89%96%E5%AD%A6</span></center><br><center><img src="/img/2025-12-01-llm-history-intro/perceptron.png" style="width:100%;"></center><br><p>感知器模仿了人类神经元（neuron）的机制：在树突接收多个输入信息（input features），经过处理（加权累加+阈值决策），最后再末梢输出信息（output decision）。</p><h1 id="2-Single-Layer-Perceptron-单层感知器"><a href="#2-Single-Layer-Perceptron-单层感知器" class="headerlink" title="2. Single-Layer Perceptron 单层感知器"></a>2. Single-Layer Perceptron 单层感知器</h1><p>从上面的例子可以看出，一个感知器只能做单个的二元决策（即回答一个“是&#x2F;否”问题）。如果我们组合多个感知器，就可以同时处理多个任务的决策，这就构成了<span style="color:tomato;"><strong>单层感知器（Single-Layer Perceptron，SLP）</strong></span>。</p><p>在上面“是否出门散步”的决策任务基础上，我们再加一个同样是二分类的任务“是否做饭”，需要两个感知器：</p><div style="text-align:center;"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -9.697ex;" xmlns="http://www.w3.org/2000/svg" width="33.771ex" height="20.526ex" role="img" focusable="false" viewBox="0 -4786.2 14926.7 9072.4" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-1-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-1-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path><path id="MJX-1-TEX-S3-7B" d="M618 -943L612 -949H582L568 -943Q472 -903 411 -841T332 -703Q327 -682 327 -653T325 -350Q324 -28 323 -18Q317 24 301 61T264 124T221 171T179 205T147 225T132 234Q130 238 130 250Q130 255 130 258T131 264T132 267T134 269T139 272T144 275Q207 308 256 367Q310 436 323 519Q324 529 325 851Q326 1124 326 1154T332 1205Q369 1358 566 1443L582 1450H612L618 1444V1429Q618 1413 616 1411L608 1406Q599 1402 585 1393T552 1372T515 1343T479 1305T449 1257T429 1200Q425 1180 425 1152T423 851Q422 579 422 549T416 498Q407 459 388 424T346 364T297 318T250 284T214 264T197 254L188 251L205 242Q290 200 345 138T416 3Q421 -18 421 -48T423 -349Q423 -397 423 -472Q424 -677 428 -694Q429 -697 429 -699Q434 -722 443 -743T465 -782T491 -816T519 -845T548 -868T574 -886T595 -899T610 -908L616 -910Q618 -912 618 -928V-943Z"></path><path id="MJX-1-TEX-N-A0" d=""></path><path id="MJX-1-TEX-N-69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z"></path><path id="MJX-1-TEX-N-66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z"></path><path id="MJX-1-TEX-N-3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-1-TEX-N-2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0,4036.2)"><g data-mml-node="mtd" transform="translate(25,0)"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="mn" transform="translate(498,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="mtd" transform="translate(926.6,0)"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(277.8,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(1333.6,0)"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(500,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(778,0)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="msub" transform="translate(3036.2,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(4267,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="msub" transform="translate(5267.2,0)"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(500,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(778,0)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g></g><g data-mml-node="msub" transform="translate(6969.9,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(8200.7,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="msub" transform="translate(9200.9,0)"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(500,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(778,0)"><use data-c="33" xlink:href="#MJX-1-TEX-N-33"></use></g></g></g><g data-mml-node="msub" transform="translate(10903.6,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><use data-c="33" xlink:href="#MJX-1-TEX-N-33"></use></g></g><g data-mml-node="mo" transform="translate(12134.4,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="msub" transform="translate(13134.6,0)"><g data-mml-node="mi"><use data-c="1D44F" xlink:href="#MJX-1-TEX-I-1D44F"></use></g><g data-mml-node="mn" transform="translate(462,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g></g></g><g data-mml-node="mtr" transform="translate(0,1999.5)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-1-TEX-I-1D466"></use></g><g data-mml-node="mo" transform="translate(300.6,16) translate(-250 0)"><use data-c="5E" xlink:href="#MJX-1-TEX-N-5E"></use></g></g></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="mtd" transform="translate(926.6,0)"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(277.8,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mrow" transform="translate(1333.6,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><use data-c="7B" xlink:href="#MJX-1-TEX-S3-7B"></use></g><g data-mml-node="mtable" transform="translate(750,0)"><g data-mml-node="mtr" transform="translate(0,600)"><g data-mml-node="mtd"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mtext" transform="translate(500,0)"><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0"></use><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">（</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">出</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">门</text><text data-variant="normal" transform="translate(3250,0) scale(1,-1)" font-size="884px" font-family="serif">散</text><text data-variant="normal" transform="translate(4250,0) scale(1,-1)" font-size="884px" font-family="serif">步</text><text data-variant="normal" transform="translate(5250,0) scale(1,-1)" font-size="884px" font-family="serif">）</text></g></g><g data-mml-node="mtd" transform="translate(7750,0)"><g data-mml-node="mtext"><use data-c="69" xlink:href="#MJX-1-TEX-N-69"></use><use data-c="66" xlink:href="#MJX-1-TEX-N-66" transform="translate(278,0)"></use><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0" transform="translate(584,0)"></use></g><g data-mml-node="msub" transform="translate(834,0)"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="mn" transform="translate(498,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(2013.3,0)"><use data-c="3E" xlink:href="#MJX-1-TEX-N-3E"></use></g><g data-mml-node="mn" transform="translate(3069.1,0)"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0,-600)"><g data-mml-node="mtd"><g data-mml-node="mn"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g><g data-mml-node="mtext" transform="translate(500,0)"><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0"></use><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">（</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">宅</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">）</text></g></g><g data-mml-node="mtd" transform="translate(7750,0)"><g data-mml-node="mtext"><use data-c="69" xlink:href="#MJX-1-TEX-N-69"></use><use data-c="66" xlink:href="#MJX-1-TEX-N-66" transform="translate(278,0)"></use><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0" transform="translate(584,0)"></use></g><g data-mml-node="msub" transform="translate(834,0)"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="mn" transform="translate(498,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(2013.3,0)"><use data-c="2264" xlink:href="#MJX-1-TEX-N-2264"></use></g><g data-mml-node="mn" transform="translate(3069.1,0)"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g></g></g></g><g data-mml-node="mo" transform="translate(12069.1,0) translate(0 250)"></g></g></g></g><g data-mml-node="mtr" transform="translate(0,0)"><g data-mml-node="mtd" transform="translate(926.6,0)"></g></g><g data-mml-node="mtr" transform="translate(0,-1300)"><g data-mml-node="mtd" transform="translate(25,0)"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="mn" transform="translate(498,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g></g><g data-mml-node="mtd" transform="translate(926.6,0)"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(277.8,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(1333.6,0)"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(500,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(778,0)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g></g><g data-mml-node="msub" transform="translate(3036.2,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(4267,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="msub" transform="translate(5267.2,0)"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(500,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(778,0)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g></g><g data-mml-node="msub" transform="translate(6969.9,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(8200.7,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="msub" transform="translate(9200.9,0)"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="TeXAtom" transform="translate(749,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(500,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(778,0)"><use data-c="33" xlink:href="#MJX-1-TEX-N-33"></use></g></g></g><g data-mml-node="msub" transform="translate(10903.6,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><use data-c="33" xlink:href="#MJX-1-TEX-N-33"></use></g></g><g data-mml-node="mo" transform="translate(12134.4,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="msub" transform="translate(13134.6,0)"><g data-mml-node="mi"><use data-c="1D44F" xlink:href="#MJX-1-TEX-I-1D44F"></use></g><g data-mml-node="mn" transform="translate(462,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g></g></g><g data-mml-node="mtr" transform="translate(0,-3336.7)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-1-TEX-I-1D466"></use></g><g data-mml-node="mo" transform="translate(300.6,16) translate(-250 0)"><use data-c="5E" xlink:href="#MJX-1-TEX-N-5E"></use></g></g></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g></g><g data-mml-node="mtd" transform="translate(926.6,0)"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(277.8,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mrow" transform="translate(1333.6,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><use data-c="7B" xlink:href="#MJX-1-TEX-S3-7B"></use></g><g data-mml-node="mtable" transform="translate(750,0)"><g data-mml-node="mtr" transform="translate(0,600)"><g data-mml-node="mtd"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mtext" transform="translate(500,0)"><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0"></use><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">（</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">做</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">饭</text><text data-variant="normal" transform="translate(3250,0) scale(1,-1)" font-size="884px" font-family="serif">）</text></g></g><g data-mml-node="mtd" transform="translate(6750,0)"><g data-mml-node="mtext"><use data-c="69" xlink:href="#MJX-1-TEX-N-69"></use><use data-c="66" xlink:href="#MJX-1-TEX-N-66" transform="translate(278,0)"></use><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0" transform="translate(584,0)"></use></g><g data-mml-node="msub" transform="translate(834,0)"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="mn" transform="translate(498,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(2013.3,0)"><use data-c="3E" xlink:href="#MJX-1-TEX-N-3E"></use></g><g data-mml-node="mn" transform="translate(3069.1,0)"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g></g></g><g data-mml-node="mtr" transform="translate(0,-600)"><g data-mml-node="mtd"><g data-mml-node="mn"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g><g data-mml-node="mtext" transform="translate(500,0)"><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0"></use><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">（</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">叫</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">外</text><text data-variant="normal" transform="translate(3250,0) scale(1,-1)" font-size="884px" font-family="serif">卖</text><text data-variant="normal" transform="translate(4250,0) scale(1,-1)" font-size="884px" font-family="serif">）</text></g></g><g data-mml-node="mtd" transform="translate(6750,0)"><g data-mml-node="mtext"><use data-c="69" xlink:href="#MJX-1-TEX-N-69"></use><use data-c="66" xlink:href="#MJX-1-TEX-N-66" transform="translate(278,0)"></use><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0" transform="translate(584,0)"></use></g><g data-mml-node="msub" transform="translate(834,0)"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-1-TEX-I-1D467"></use></g><g data-mml-node="mn" transform="translate(498,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(2013.3,0)"><use data-c="2264" xlink:href="#MJX-1-TEX-N-2264"></use></g><g data-mml-node="mn" transform="translate(3069.1,0)"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g></g></g></g><g data-mml-node="mo" transform="translate(11069.1,0) translate(0 250)"></g></g></g></g></g></g></g></svg></mjx-container></div><br><center><img src="/img/2025-12-01-llm-history-intro/slp.png" style="width:80%;"></center><br><ul><li>在只有单个感知器时，有 3 个输入特征，用到了 3 个 weights 和 1 个 bias，得到 1 个输出结果；</li><li>在 2 个感知器组成的 SLP 里，同样还是 3 个输入特征，用到了 6 个 weights 和 2 个 bias，得到 2 个输出结果。</li></ul><p>那么在 SLP 这个结构里，每多一个感知器，就多 N+1 个参数，N 是输入特征的数量。</p><p>Frank Rosenblatt 在 1958 年发表的论文提出的单层感知器，具有很大的局限性，它无法处理 <u>线性不可分</u> 问题。以上的举例都是理想化的线性可分任务，能为模型找到合适的参数（ <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.36ex" height="1.359ex" role="img" focusable="false" viewBox="0 -443 1043 600.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D464" xlink:href="#MJX-1-TEX-I-1D464"></use></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g></g></svg></mjx-container> 和 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D44F" xlink:href="#MJX-1-TEX-I-1D44F"></use></g></g></g></svg></mjx-container> ），但现实中是基本不存在线性可分这种理想情况。</p><h1 id="3-Multi-Layer-Perceptron-多层感知器"><a href="#3-Multi-Layer-Perceptron-多层感知器" class="headerlink" title="3. Multi-Layer Perceptron 多层感知器"></a>3. Multi-Layer Perceptron 多层感知器</h1><p>Frank Rosenblatt 当时也提出了在 SLP 的输入层（input layer）和输出层（output layer）外，再加入隐藏层（hidden layer）构成<span style="color:tomato;"><strong>多层感知器（Multi-Layer Perceptron，MLP）</strong></span>，来解决线性不可分问题。然而当时缺乏有效的训练手段，使这个想法一度停留在理论层面。直到接近 30 年后的 1986 年，David E. Rumelhart、Geoffrey E. Hinton 和 Ronald J. Williams 提出用反向传播（backpropagation）训练 MLP，才使得 MLP 可以实现。</p><p><span style="color: gray; font-size:12px;"><span style="color: white; background-color: violet; padding: 0 2px;"><b>小注</b></span> 在反向传播出现前，单层感知器依靠简单的学习规则更新权重，只能处理线性可分问题；反向传播利用了微积分的链式法则，使得多层网络得以有效训练。这里我们只谈论模型结构，不展开叙述训练方法。有兴趣的可以去看 Andrew NG 的机器学习相关课程，一步步教你手推反向传播。</span></p><br><center><img src="/img/2025-12-01-llm-history-intro/mlp.png" style="width:100%;"></center><br><p>上图左边是 SLP，右边是加入了一个隐藏层的 MLP。每一条连接线代表 1 个 weight，每个输出神经元（隐藏层和输出层）带有 1 个 bias。</p><p>对于 3 个神经元的输入层和 2 个神经元的输出层的 SLP，模型的参数总量是 8。</p><blockquote><p><span style="font-size:12px;">输入层 -&gt; 输出层：权重 w（2×3&#x3D;<span style="color:violet;"><b>6</b></span>），偏置 b（<span style="color:violet;"><b>2</b></span>）</span><br>6+2&#x3D;<font style="color:violet;"><b>8</b></font></p></blockquote><p>当我们加入了 4 个神经元作为一个隐藏层（3 → 4 → 2），参数总量达到了 26，是 SLP 的三倍多。</p><blockquote><p><span style="font-size:12px;">输入层 -&gt; 隐藏层：权重 w（4×&#x3D;<span style="color:violet;"><b>12</b></span>），偏置 b（<span style="color:violet;"><b>4</b></span>）</span><br><span style="font-size:12px;">隐藏层 -&gt; 输出层：权重 w（2×&#x3D;<span style="color:violet;"><b>8</b></span>），偏置 b（<span style="color:violet;"><b>2</b></span>）</span><br>12+4+8+2&#x3D;<font style="color:violet;"><b>26</b></font></p></blockquote><p>可以想象一下，如果输入层有 100 个神经元，同样的 MLP 结构（100 → 4 → 2），参数总量则会有 414。</p><blockquote><p><span style="font-size:12px;">输入层 -&gt; 隐藏层：权重 w（4×100&#x3D;<span style="color:violet;"><b>400</b></span>），偏置 b（<span style="color:violet;"><b>4</b></span>）</span><br><span style="font-size:12px;">隐藏层 -&gt; 输出层：权重 w（2×4&#x3D;<span style="color:violet;"><b>8</b></span>），偏置 b（<span style="color:violet;"><b>2</b></span>）</span><br>400+4+8+2&#x3D;<font style="color:violet;"><b>414</b></font></p></blockquote><br><p>在实际应用中，输入层的神经元数量远不止 100 个。例如：</p><p><span style="color:#2D9966; font-size: 14px;"><b>图像处理</b></span></p><blockquote style="padding: 5px; border-radius: 2px; border-left: none; outline: 1px dotted green;">    <ul style="color: #2D9966; font-size: 14px; padding-left: 1.5em;">    <li>一张 28×28 的灰度手写数字图片，输入层就有 784 个神经元（28×28=784）。</li>    <li>如果是彩色图片，每个像素有 3 个通道，输入层的神经元数量会再乘以 3（784×3=2352）。</li>    <li>我们手机拍摄图片的像素现在大多去到百万像素级别。2024 年手机主摄像头的平均分辨率约为 54 MP，也就是 54 百万像素，那么模型要处理我们手机里的一张图片，输入层就有 <b style="color: tomato;">1 亿 6200 万</b>（54,000,000×3=162,000,000）个神经元。</li>    </ul></blockquote><p><span style="color:#2D9966; font-size: 14px;"><b>文本处理</b></span></p><blockquote style="padding: 5px; border-radius: 2px; border-left: none; outline: 1px dotted green;">    <ul style="color: #2D9966; font-size: 14px; padding-left: 1.5em;">    <li>在自然语言处理任务中，并不是把文本数据直接丢进模型。</li>    <li>比如这句话 “<i style="color: gray;">人人期望可达到，我的快乐比天高。</i>” 表面上是长度为 16 的字符串，但进入模型前需要先切分成 token（不同的编码方式 token 长度会有差异，比如这句话在 GPT-4o 的 token 长度是 12，在 GPT-4 的 token 长度是 18），接着每个 token 会被映射成一个高维向量。假设 token 长度是 12，映射成最常见的 768 维，那么这 16 个字在输入层就有 9216（12×768=9216）个神经元。</li>    <li>如果是一篇 1000 字的文章，假设切成 600 token，那么输入层会有 <b style="color: tomato;">46 万</b>多（600×768=460,800）个神经元。</li>    </ul></blockquote><p>不管是图像还是文本，输入层的神经元数量可能有几万甚至上亿，光是第一层就让参数量暴涨。如果只有一层或两层隐藏层，模型不足以学习出这些复杂数据里的规律。那么我们就要增加隐藏层，让模型一层一层地抽象和组合特征，逐步学会数据的规律。隐藏层越多，模型就变“深”了，这类模型就是我们说的<span style="color:tomato;"><b>深度神经网络（Deep Neural Networks，DNN）</b></span>。现在所说的<span style="color:tomato;"><b>深度学习（Deep Learning，DL）</b></span>，本质上指的就是使用这些 DNN 的模型。</p><center><img src="/img/2025-12-01-llm-history-intro/dnn.png" style="width:80%;"><span  style="display:block; font-size:12px; color:gray; text-align:left;  word-break:break-all;">http://neuralnetworksanddeeplearning.com/chap5.html</span></center><br><p>MLP 在 1986 年就能被实现了，然而在当时并没有得到广泛应用。硬件的算力不足以支撑如此大规模的运算，可以获取到的训练数据也非常有限，而且反向传播在当时只能有效训练 1 到 2 层隐藏层，再深的网络容易出现梯度消失或爆炸的问题，实际训练非常困难。</p><p>到了 2006 年左右，随着 GPU 并行计算能力的提升，以及互联网带来的海量文本、图片等数据和各类公开标准数据集（如 MNIST、ImageNet）的积累，再加上 Hinton 等人提出的深度置信网络（Deep Belief Networks，DBN）预训练方法，深层神经网络才得以高效训练，DNN 开始流行，开启了现代<span style="color:tomato;"><b>人工神经网络（Artificial Neural Networks，ANN）</b></span>的新篇章。</p><h1 id="4-Artificial-Neural-Networks-人工神经网络"><a href="#4-Artificial-Neural-Networks-人工神经网络" class="headerlink" title="4. Artificial Neural Networks 人工神经网络"></a>4. Artificial Neural Networks 人工神经网络</h1><p>虽然 DNN 已经可以学习复杂特征，但对于现实任务还是有明显的不足：它会把每个像素当成独立特征处理，忽略了像素之间的空间关系；把每个 token 当做独立输入，难以捕捉词间的顺序和上下文关系。如果把一张高分辨率图像或者是一篇长文本直接丢给 DNN，参数量暴涨，训练难度极大。为了解决这些问题，科学家们又提出了<span style="color:tomato;"><b>卷积神经网络（Convolutional Neural Networks，CNN）</b></span>和<span style="color:tomato;"><b>循环神经网络（Recurrent Neural Networks，RNN）</b></span>，前者擅长提取局部空间特征，后者能够捕捉序列中的上下文信息。CNN 和 RNN 成为了 DNN 在 CV 领域（Computer Vision）和 NLP 领域（Natural Language Processing）的高效变体。</p><blockquote style="padding: 15px; border-radius: 2px; border-left: none; outline: 2px dotted green;"><center><img src="/img/2025-12-01-llm-history-intro/cnn.png" style="width:100%;"><span  style="display:block; font-size:12px; color:gray; text-align:left;  word-break:break-all;">https://ravjot03.medium.com/decoding-cnns-a-beginners-guide-to-convolutional-neural-networks-and-their-applications-1a8806cbf536</span></center><p>CNN（卷积神经网络），在传统 DNN 的基础上加入了卷积层（Convolutional Layer）和池化层（Pooling Layer），不再把每个像素当成孤立特征，而是用“小窗口”在图像里滑动，逐块提取局部结构，让模型真正理解空间关系。卷积层负责找特征（如边缘、角点、纹理），池化层则负责压缩和浓缩信息，使模型更稳健、参数更少。这样的设计让 CNN 在处理图像时既高效又聪明，成为 CV 领域的核心架构。<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="💬 无言以对的故事碎片：旁观过一个项目要做物体识别，业务负责人提出要标注不规则形状的物体。我本以为我们的技术人员会告诉他，需要用方块区域标注，模型会通过滑动窗口判断物体边缘。结果，第二天大无语，我看到服务平台上多了一张工单，技术人员向服务商提出要标注不规则形状的需求。其实，哪怕是做管理或业务，学一点技术原理可以有很大帮助，避免提出不合理需求，节省大家的时间和精力。">[1]</span></a></sup></p></blockquote><blockquote style="padding: 15px; border-radius: 2px; border-left: none; outline: 2px dotted green;"><center><img src="/img/2025-12-01-llm-history-intro/rnn.png" style="width:100%;"><span  style="display:block; font-size:12px; color:gray; text-align:left;  word-break:break-all;">https://magenta.withgoogle.com/2016/06/10/recurrent-neural-network-generation-tutorial</span></center>RNN（循环神经网络），在 DNN 的基础上引入了循环结构，能够保留序列中的历史信息。它不像普通 DNN 那样把每个 token 当作独立输入，而是一边处理当前输入，一边参考前面的状态，逐步积累上下文信息。这样的设计让 RNN 擅长处理时间序列或文本数据，能够捕捉词间顺序和长短期依赖，是 NLP 和语音处理中的重要架构。</blockquote><p>CNN、RNN 以及 RNN 的变体 LSTM，这些现代深度学习常见的基础架构，其实理论基础都在 1980s-1990s 就已经提出了<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="💬 瞠目结舌的故事碎片：17 年左右跟领导去参加一个 AI 分享会，演讲人问大家是什么时候第一次听到人工神经网络的。我心想，那不就是这几年的事。结果我那退休返聘的领导说他在 90 年代听过。我才知道，原来我眼里的“新兴科技”其实在三十年前就已经有这些概念了。">[2]</span></a></sup>，不过当时受限于算力和数据量，没有得到广泛应用。前面提到在 2000s 后半算力提升和互联网推动的数据积累，这些架构才全面复兴。</p><center><img src="/img/2025-12-01-llm-history-intro/transformer.png" style="width:100%;"><span  style="display:block; font-size:12px; color:gray; text-align:left;  word-break:break-all;">https://medium.com/data-science/transformers-89034557de14</span></center><p>又到了 2017 年，<span style="color:tomato;"><b>Transformer</b></span> 的出现又给 AI 技术带来了新的高度。Transformer 是为了解决序列数据中（比如文本）长期依赖和训练效率问题的模型架构。它摒弃了传统 RNN 的逐步计算，通过自注意力机制（Self-Attention）同时考虑序列中所有位置的关系，让模型能捕捉长距离依赖，同时支持高度并行化训练。结构上主要由编码器（Encoder）和解码器（Decoder）堆叠组成，每个模块包含注意力层和前馈神经网络层，注意力机制决定了序列中各个位置信息的权重分配，从而实现对上下文的灵活理解。</p><p>NLP 可以分成两个子领域：</p><ul><li>NLU（Natural Language Understanding）自然语言理解：文本分类、命名实体识别、情感识别、意图识别、句法分析、关键词抽取……</li><li>NLG（Natural Language Generation）自然语言生成：文本生成</li></ul><p>这两类任务的底层技术路径不太一样。</p><p>在 NLU 方面，2018 年 Google 开源了 BERT 模型，把 Transformer 双向编码器改造成可预训练的大规模语言模型，一下子成为香饽饽。开发者直接下载 pre-trained model（预训练模型），进行 fine-tuning（微调）就可以让各种下游 NLP 任务获得比之前更好的效果。虽然 BERT 在参数规模上还不算如今意义上的“大模型”，但它开启了通用 pre-trained model 的时代。</p><p>与此同时，在 NLG 方面，GPT 系列（Generative Pre-trained Transformer）也用了 Transformer 架构，但采用单向自回归方式生成文本，专注于生成能力。GPT-1 于 2018 年发布，标志着生成式模型的发展起点。</p><center><img src="/img/2025-12-01-llm-history-intro/gpt_3_params.png" style="width:100%;"><span  style="display:block; font-size:12px; color:gray; text-align:left;  word-break:break-all;">[Paper] Language Models are Few-Shot Learners</span></center><br><p>GPT-1 的参数规模是 1.17 亿，到了 2019 年 GPT-2 的参数规模达到了 15 亿，再到了 2020 年发布的 GPT-3 参数规模竟然膨胀到了 1750 亿。语言模型（Language Model）的参数规模居然可以堆到这个量级！随即大语言模型（Large Language Model，LLM）开始在业界被高频讨论，各路研究争先碰瓷 GPT-3。<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="💬 我的记仇故事碎片：22 年还没有 ChatGPT 的时候，老大推了一个生成式模型让我们研究一下。当时 GPT-3 是生成式模型里领先的，据说这个模型能超越 GPT-3。我上网搜了一下相关信息，内外全网只找到一个在知乎的提问帖——[如何评价浪潮发布的2457亿参数源1.0智能大模型？与GPT-3相比如何？处于AI模型领域什么水平？](https://www.zhihu.com/question/504163853)回答全都自称“利益相关”，评论由作者筛选显示，都是一堆没头像的小号在称赞。我是觉得不妙，但不可抗力非让我研究。一上手就发现问题，只有官方给的样例能得到稍微像样的结果，输入内容变化了一个字就会生成奇奇怪怪的东西。不得已上 GitHub 上提了 issue，一通下来浪费了我不少时间精力，最终毫无收获。（现在还能看到这个 issue：[https://github.com/Shawn-IEITSystems/Yuan-1.0/issues/30](https://github.com/Shawn-IEITSystems/Yuan-1.0/issues/30)。最后一怒之下我还改了标题说它是 rubbish。）">[3]</span></a></sup></p><p>到了 2022 年 11 月，OpenAI 发布了面向大众的产品 ChatGPT，从此把 LLM 带进了大众视野，引发了之后各大厂商的竞争。</p><h1 id="〰️✨结语✨〰️"><a href="#〰️✨结语✨〰️" class="headerlink" title="〰️✨结语✨〰️"></a>〰️✨结语✨〰️</h1><p>现在我们所谈论的 AI 算法，大多指的是人工神经网络模型，是源于感知器发展而来的复杂模型。它们虽然借鉴了生物神经元的概念，但本质上是数学模型，基于概率和统计进行海量计算跑出来的结果，而不是在模拟人类大脑的真实结构。</p><p>其实在 Transformer 出现之后，业界并没有再冒出什么突破性的新技术。现在大模型的进化可以说是靠“大力出奇迹”，很大程度上依赖于算力和数据质量，堆参数、堆更干净更丰富的语料，再加上和外部的各种交互方式。期待未来还会有新的突破出现，或许应该大概也许不至于又要等一个 30 年吧。</p><hr><span style="color: gray; font-size: 35px;"><p></span><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>💬 无言以对的故事碎片：旁观过一个项目要做物体识别，业务负责人提出要标注不规则形状的物体。我本以为我们的技术人员会告诉他，需要用方块区域标注，模型会通过滑动窗口判断物体边缘。结果，第二天大无语，我看到服务平台上多了一张工单，技术人员向服务商提出要标注不规则形状的需求。其实，哪怕是做管理或业务，学一点技术原理可以有很大帮助，避免提出不合理需求，节省大家的时间和精力。 ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>💬 瞠目结舌的故事碎片：17 年左右跟领导去参加一个 AI 分享会，演讲人问大家是什么时候第一次听到人工神经网络的。我心想，那不就是这几年的事。结果我那退休返聘的领导说他在 90 年代听过。我才知道，原来我眼里的“新兴科技”其实在三十年前就已经有这些概念了。<br><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>💬 我的记仇故事碎片：22 年还没有 ChatGPT 的时候，老大推了一个生成式模型让我们研究一下。当时 GPT-3 是生成式模型里领先的，据说这个模型能超越 GPT-3。我上网搜了一下相关信息，内外全网只找到一个在知乎的提问帖——<a href="https://www.zhihu.com/question/504163853">如何评价浪潮发布的2457亿参数源1.0智能大模型？与GPT-3相比如何？处于AI模型领域什么水平？</a>回答全都自称“利益相关”，评论由作者筛选显示，都是一堆没头像的小号在称赞。我是觉得不妙，但不可抗力非让我研究。一上手就发现问题，只有官方给的样例能得到稍微像样的结果，输入内容变化了一个字就会生成奇奇怪怪的东西。不得已上 GitHub 上提了 issue，一通下来浪费了我不少时间精力，最终毫无收获。（现在还能看到这个 issue：<a href="https://github.com/Shawn-IEITSystems/Yuan-1.0/issues/30">https://github.com/Shawn-IEITSystems/Yuan-1.0/issues/30</a>。最后一怒之下我还改了标题说它是 rubbish。）<br><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section></p>]]></content>
    
    
    <categories>
      
      <category>Tech &amp; Life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI Behind the Scenes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>揭开大模型的假开源遮羞布</title>
    <link href="/techlife/llm-open-source-vs-open-weight.html"/>
    <url>/techlife/llm-open-source-vs-open-weight.html</url>
    
    <content type="html"><![CDATA[<blockquote style="font-size: 14px;color: White; background-color: darkslategray; padding: 15px; border-radius: 10px; outline: 2px dotted darkslategray; outline-offset: 5px;">    <b style="font-size: 16px;">TL;DR</b>    <ul style="margin: 10px 0; padding-left: 1.5em;">        <li>Meta 发布的所谓“开源” Llama 2 其实是一种营销手段，它根本没有开源。大厂的“开源”大模型都是假开源。</li>        <li>大模型开源开了什么？Open Weight 和 Open Source 有什么区别。</li>        <li>Open Weight 和 Open Source 怎么选？</li>    </ul></blockquote><br><div style="text-align: center; font-size: 16px; color: gray;">· · · · · ✦ ✧ ✦ · · · · ·</div><br><h1 id="假开源吃真红利"><a href="#假开源吃真红利" class="headerlink" title="假开源吃真红利"></a>假开源吃真红利</h1><p>Meta 在 2023 年 7 月发布了 Llama 2，LLM 大众化后首个来自大厂的“开源”大模型。瞬间引发了关注。非从业者跃跃欲试，想要追上 AI 的风口，在蓝海里分一杯羹。从业者则很疑虑怎么会开源，因为模型的训练数据很可能包含来自网络爬取、并非完全通过正规或授权渠道获取的内容，公开训练数据极有可能引发合规争议。</p><p>很快大家就发现了 Llama 2 的“开源”谎言。没有公开训练数据集，没有公开源码，开发者根本没有办法从零复现一个 Llama 2。Llama 2 依旧是一个黑盒子。简单来说，这类似是开放了一个可以离线运行的软件。</p><center><img src="/img/2025-12-01-llm-open-source-vs-open-weight/maomaotou.jpg" style="width:20%;"></center><br><p>Llama 2 发布两天后，OSI（Open Source Initiative，开源促进会）发布了一篇文章《<a href="https://opensource.org/blog/metas-llama-2-license-is-not-open-source">Meta’s LLaMa license is not Open Source</a>》。指出 Llama 2 并非开源，因为它违反了 OSD（Open Source Definiton）。希望 Meta 能更正相关表述。</p><p>到了 2025 年 2 月，OSI 又发布了一篇文章《<a href="https://opensource.org/blog/metas-llama-license-is-still-not-open-source">Meta’s LLaMa license is still not Open Source</a>》。指出 Meta 发布的新模型 Llama 3 依然不属于开源，而且还继续在误导性地宣传自己是开源。呼吁大家去催小扎改 Llama 许可，让它符合开源定义。</p><p>现在是 2025 年 11 月，我们到 Llama 的官网可以看到 Llama 3 页面的副标题依然明晃晃地写着 open-source。</p><center><img src="/img/2025-12-01-llm-open-source-vs-open-weight/llama3.png" style="width:60%;"><span style="display:block; font-size:12px; color:gray; text-align:left;  word-break:break-all;">https://www.llama.com/models/llama-3/</span></center><br><p>OpenAI 打开了 LLM 大众化的市场后，一直是以 API 提供模型应用，完全是闭源的黑盒子，被社区戏谑为 CloseAI。Meta 抓住了闭源垄断的矛盾点，借助 Llama 2“开源”将自己塑造成开放积极的形象，赢得了大量开发者的支持。允许用户免费下载模型并在本地运行，吸引了海量创新涌入 Llama 生态，所有基于 Llama 的应用探索，都成为 Meta 免费享用的生态成果。</p><p>之后不少厂商都学 Meta 打着“开源”的旗号抢占市场。反正应用层无人在意是不是真开源。</p><h1 id="Open-Weight-🆚-Open-Source"><a href="#Open-Weight-🆚-Open-Source" class="headerlink" title="Open Weight 🆚 Open Source"></a>Open Weight 🆚 Open Source</h1><p>在软件许可法律专家和社区共建的努力下，经过一年时间，OSI 在 2024 年 10 月发布了 OSAID（Open Source AI Definition）把开源 AI 的定义补丁打上。《<a href="https://opensource.org/blog/the-open-source-initiative-announces-the-release-of-the-industrys-first-open-source-ai-definition">The Open Source Initiative Announces the Release of the Industry’s First Open Source AI Definition</a>》这篇文章指出 AI 模型必须公开足够多的训练数据来源或方法，让专业人士能够复现一个等效模型，才能称为 Open Source。</p><blockquote><p>OSAID 1.0：<a href="https://opensource.org/ai/open-source-ai-definition">https://opensource.org/ai/open-source-ai-definition</a></p></blockquote><p>OSI 还给出了 Open Weights（开放权重）和 Open Source（开源）的对比：</p><center><img src="/img/2025-12-01-llm-open-source-vs-open-weight/open_weight_open_source_notation.jpg" style="width:100%;"><span style="display:block; font-size:12px; color:gray; text-align:left;  word-break:break-all;">https://opensource.org/ai/open-weights</span></center><p><span style="color: gray; font-size:12px;"><span style="color: white; background-color: violet; padding: 0 2px;"><b>小注</b></span> 模型参数的 weights 和 biases，想知道概念可以看这篇：《<a href="/techlife/llm-history-intro.html" title="大模型的前世今生">大模型的前世今生</a>》。</span></p><br> <p>基本上常见的所谓开源 LLM，都是 open weight。比如 Llama（Meta），Mistral（Mistral），Phi（Microsoft），Gemma（Google），gpt-oss（OpenAI），DeepSeek（DeepSeek），Qwen（阿里），GLM（智谱），Kimi K2（月之暗面）。</p><p>OpenAI 在 2025 年 8 月发布了 gpt-oss 模型，倒是用了 open-weight 这个描述。</p><center><img src="/img/2025-12-01-llm-open-source-vs-open-weight/gpt_oss.png" style="width:60%;"><span style="display:block; font-size:12px; color:gray; text-align:left;  word-break:break-all;">https://openai.com/index/introducing-gpt-oss/</span></center><br><h1 id="真正的开源-AI"><a href="#真正的开源-AI" class="headerlink" title="真正的开源 AI"></a>真正的开源 AI</h1><p>那有没有真正开源的 AI 呢？OSI 也是有列举的：</p><center><img src="/img/2025-12-01-llm-open-source-vs-open-weight/true_open_source.png" style="width:100%;"><span style="display:block; font-size:12px; color:gray; text-align:left;  word-break:break-all;">https://opensource.org/ai#share</span></center><br><h1 id="认清自己，慎选真开源"><a href="#认清自己，慎选真开源" class="headerlink" title="认清自己，慎选真开源"></a>认清自己，慎选真开源</h1><p>有的人特别有技术追求，总喜欢上难度以显得自己更🐮🐝。越 raw 越好，越 raw 就代表技术含量越高。如果你清楚领导是这种风格，那最好就别让他知道还有真开源这回事。</p><p>刚才说到应用层无人在意是不是真开源，这是因为<strong>对应用层来说，open weight 是比 open source 更合适的解决方案</strong>。这个可以离线运行的软件只需要传统的开发岗就可以做到落地应用。开源虽假，但它<span style="color: tomato;">支持商用，支持本地部署</span>，这就够了。相对于完全封装的 API，open weight 带来了很多优点：成本可控、延迟可控、数据安全、离线能力、资源分配可控。（在 inference 推理方面，和 API 调用的方式差不多，详情可见《<a href="/techlife/llm-application-basic.html" title="LLM应用💫不需要学AI，会调接口就行">LLM应用💫不需要学AI，会调接口就行</a>》。）</p><p>用 open source 确实会显得技术上更🐮🐝，换言之，对研发团队的要求更高。你得有懂深度学习的机器学习工程师、懂算力和部署的 MLOps 工程师、会数据清洗和建模的数据科学家，还得有人负责工作量又大又枯燥的数据标注，整个链条缺一不可。除了大量的人力需求还有昂贵的硬件需求。总的来说，如果你是研究人员，<strong>打算从零开始研发一个 LLM，open source 是符合你的期待的</strong>。</p><h1 id="〰️✨结语✨〰️"><a href="#〰️✨结语✨〰️" class="headerlink" title="〰️✨结语✨〰️"></a>〰️✨结语✨〰️</h1><p>大模型的“开源”已经改写了以往我们对“开源”的定义。我们想要从现在所谓的“开源”大模型中拆解点什么来复现一个自己的大模型，无异于管中窥豹，难探究竟。基于成熟模型做好下游应用，更为现实。</p>]]></content>
    
    
    <categories>
      
      <category>Tech &amp; Life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI Behind the Scenes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我好像并不挑食</title>
    <link href="/moments/not-really-a-picky-eater.html"/>
    <url>/moments/not-really-a-picky-eater.html</url>
    
    <content type="html"><![CDATA[<p>小时候家里人总说我太挑食。我不吃苦瓜，不吃胡萝卜，不吃鱼。<br>妈妈总是抱怨，为了迁就我，她每次做饭都很为难。带我去亲戚家里吃饭，不合口味的菜我一口都不吃，她觉得很丢脸。</p><p>后来到广州上大学，学校食堂有部分窗口是承包给第三方的，除了乏善可陈的食堂菜，可以吃到拉面、蛋包饭、牛扒猪扒、荷叶蒸饭、瓦罐汤……在大学城里还可以试试其他学校的食堂，都是刷一卡通非常方便。<br>学校附近的露天广场汇聚了天南地北的小吃摊。酱香饼、麻辣烫、珍珠奶茶、寿司卷、烧仙草、煎饼果子、章鱼小丸子……<br>其实，不吃苦瓜不吃胡萝卜不吃鱼，还是有很多食物可以选的嘛～</p><p>再后来到欧洲留学，对食物的探索又到了 next level。<br>在芝士大国被安利了各种芝士，我确定了自己无法爱上。我不嗜甜，欧洲甜品的甜度能够让我精神恍惚。<br>法式食堂菜里炖得没入口就已经化了的大葱，给从没吃过大葱的我带来了小小的冲击。当然，冲击我的不是大葱，是这种把蔬菜置诸死地的炖煮程度。<br>在差异巨大的饮食文化冲击中，还是能找到舒适区的。在鼓起勇气生吃了海鲜拼盘里的生生蚝后，我发现除了炭烧蒜蓉的做法，生蚝本蚝就很美味～～～莫泊桑吃的也是这样的鲜美生蚝吧。</p><p>找到的舒适区不大，下馆子的费用也高。和大部分留学生一样，我被动触发了做饭技能。<br>在此之前我对烹饪毫无涉猎。在网上找教程跟着做了第一顿饭后，我感觉中国人大概天生就自带做饭技能，点亮即可。</p><p>现在又回到了广州生活，我在家的时候基本都是我掌勺。在点亮了做饭技能后，发现小时候所打上的“挑食”标签，其实很大可能是家里人厨艺稍逊。这回轮到我觉得他们挑食了。可乐鸡翅不爱吃，咖喱鸡肉咖喱牛肉不爱吃，土豆炖茄子不爱吃，煎牛扒不爱吃。<br>我和朋友们会去尝试川菜、湘菜、日料、泰餐、越南菜等等各种口味的菜式。我也带家里长辈去尝过一些在家吃不到的东西，但是我发现他们只接受粤菜，一些已经做了本土化改良的他们也吃不惯。<br>小时候他们认为我挑食，到头来发现我是家里对不同食物接受度最高的。</p><hr><p>首发：小红书 <a href="http://xhslink.com/o/7zNINOx8PrX">http://xhslink.com/o/7zNINOx8PrX</a></p>]]></content>
    
    
    <categories>
      
      <category>Moments</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>零代码查看重复文件的小妙招</title>
    <link href="/techlife/tip-check-duplicate-file.html"/>
    <url>/techlife/tip-check-duplicate-file.html</url>
    
    <content type="html"><![CDATA[<blockquote style="font-size: 14px;color: White; background-color: darkslategray; padding: 15px; border-radius: 10px; outline: 2px dotted darkslategray; outline-offset: 5px;">    <b style="font-size: 16px;">TL;DR</b>    <br>    要检查的文件放到同一级文件里，全选压缩成压缩文件。然后用 WinRAR 双击打开，如果 CRC32 相同则为同一文件。</blockquote><br><div style="text-align: center; font-size: 16px; color: gray;">· · · · · ✦ ✧ ✦ · · · · ·</div><br><p>之前做一个物体检测模型，在数据收集阶段遇到一个令人头大的小问题。当时用的是 GCP (Google Cloud Platform) 的低代码模型训练平台 AutoML，把图片上传到平台可以进行人工标注。平台要求每一种物体至少上传 10 张图片才能执行训练任务，基于这个数量要求我定的数据需求是在同一个位置拍摄 10 张图片。我考虑到前线人员的工作量，所以并没有要求从不同角度拍摄，只要把相关物体都拍到就可以了，站在同一个位置连续按 10 次照相机就可以完成。</p><p>即使要求已经很低了，但还是有自作聪明的懒人，拍了 1 张图片然后复制 9 张，改了名字就发过来交差。重复图片上传到 AutoML 会自动去重，也就是说我收集到 10 张图片，上传成功的只有 1 张。批量上传图片所需费时，需要等图片全部上传完才知道有重复图片被去重了。图片数量繁多，而且我并没有要求从不同角度拍摄，所以考虑从相似图片来手动剔除再上传是很难实现的。我必需解决这个费时费力的小问题。</p><p>那时候还没有 ChatGPT，当然我现在尝试了问 ChatGPT，也是给了我几个差不多的方案。一是查哈希值。每张图片有唯一的哈希值，如果哈希值一样代表是同一张图片。怎么查哈希值呢？在电脑终端写命令，或者是写 Python 调包。二是下载软件。</p><p>正打算找个 Python 代码来批量读哈希值呢，在整理图片的时候忽然另辟蹊径发现了一个另类的方式。用 WinRAR 这个压缩软件打开包含多张图片的压缩包，可以看到一列信息叫 <b  style="color: tomato;">CRC32</b>。</p><p><img src="/img/2025-07-25-winrar-check-duplicates/winrar.png"></p><p>上图第二张图片 <i  style="color: violet;">copy.jpg</i> 是第一张图片复制的副本，可以看到它俩的 CRC32 是一样的。那么把所有图片放到一起压缩，再用 WinRAR 打开，CRC32 按序排列就可以查看有哪些图片是重复的。我就不用等图片在平台上传完才知道又缺哪些位置的图片了。这样不用写代码，也不用另外下载工具，问题解决！</p><blockquote style="padding: 15px; border-radius: 10px; border-left: none; outline: 3px dotted darkslategray;">    <b style="color: tomato;">什么是 CRC32</b>    <br>    CRC（Cyclic Redundancy Check），循环冗余校验，用来快速检查数据内容有没有变化。CRC32 是 CRC 算法的一种具体实现，输出固定为 32 位（通常以 8 位十六进制显示），常见于以太网帧（Ethernet FCS）、ZIP、PNG、GZIP 等格式。</blockquote><p>所以，除了图片，其他类型的文件也可以用这种方式来查看是否重复。<br><img src="/img/2025-07-25-winrar-check-duplicates/winrar1.png"><br>比如我们有一个原始的 Word 文档 <i  style="color: violet;">测试word.docx</i>，复制了 2 个副本，其中 <i  style="color: violet;">测试word - 副本.docx</i> 不做任何改动，<i  style="color: violet;">测试word - 副本 (2).docx</i> 改动了内容再保存。可以看到没做改动的文件和原文件的 CRC32 值是一样的，而改动过的文件 CRC32 值就变了。</p>]]></content>
    
    
    <categories>
      
      <category>Tech &amp; Life</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>RAG架构💫源于AI，不只AI</title>
    <link href="/techlife/rag-concept.html"/>
    <url>/techlife/rag-concept.html</url>
    
    <content type="html"><![CDATA[<blockquote style="font-size: 14px;color: White; background-color: darkslategray; padding: 15px; border-radius: 10px; outline: 2px dotted darkslategray; outline-offset: 5px;">    <b style="font-size: 16px;">TL;DR</b>    <ul style="margin: 10px 0; padding-left: 1.5em;">        <li>影响 RAG 最终效果的不止底层的 LLM，还有 retrieve（检索）结果，如果检索结果没有找对，LLM 再厉害也得不到理想的回答。</li>        <li>不同的 chunking 会影响 retrieve 结果，如何选择 chunking 策略是一个不小的挑战。</li>        <li>最大的工作量不是在 RAG 流程搭建，而是在 chunking 和 retrieve 的策略选择。</li>    </ul></blockquote><br><div style="text-align: center; font-size: 16px; color: gray;">· · · · · ✦ ✧ ✦ · · · · ·</div><br><p>如前文《<a href="/techlife/llm-application-basic.html" title="LLM应用💫不需要学AI，会调接口就行">LLM应用💫不需要学AI，会调接口就行</a>》所说，AI 对话产品中的“联网搜索”是通过检索来优化 LLM 的生成结果，这种技术架构称为 <strong>RAG（Retrieval-Augmented Generation，检索增强生成）</strong>。现在 RAG 在业界十分常见，用于搭建基于私有知识库的 AI 场景。通过检索从企业或个人的私有知识库中提取相关信息，结合 LLM 进行生成，有效保证内容的专业性和私密性。</p><h1 id="1-Retrieve（检索）"><a href="#1-Retrieve（检索）" class="headerlink" title="1. Retrieve（检索）"></a>1. Retrieve（检索）</h1><p>在 RAG 架构中，检索环节是核心步骤，决定了生成内容的相关性和准确性。“联网搜索”功能中，数据源就是互联网数据，检索方式就是搜索引擎，直接调用搜索引擎 API 就可以解决了。那么我们的数据源换成私有知识库，检索又如何实现呢？</p><h2 id="1-1-检索策略"><a href="#1-1-检索策略" class="headerlink" title="1.1. 检索策略"></a>1.1. 检索策略</h2><p>当前很常用的检索策略是 <span style="color:orange;"><strong>Hybrid search（混合搜索）</strong></span>。Hybrid search 是指 <u>Lexical search（词汇搜索）</u>和 <u>Semantic search（语义搜索）</u>两种搜索类别的结合。</p><h3 id="1-1-1-Lexical-search（词汇搜索）"><a href="#1-1-1-Lexical-search（词汇搜索）" class="headerlink" title="1.1.1. Lexical search（词汇搜索）"></a>1.1.1. Lexical search（词汇搜索）</h3><p><span style="color:orange;"><strong>Lexical search（词汇搜索）</strong></span>，主要技术有 <span style="color:violet;">Keyword search（关键词搜索）</span>和 <span style="color:violet;">Full-text search（全文搜索）</span>。Keyword search（关键词搜索），顾名思义是找到精准匹配的关键词。而 Full-text search（全文搜索）可以看作是在关键词匹配的基础上，引入了部分传统 NLP 技术（不包括深度学习），从而支持模糊匹配和更灵活的检索。</p><p>词汇搜索的应用很常见：</p><ul><li>电商平台搜索产品（例如在淘宝搜扫地机器人可以找到相关的商品）</li><li>博客或知识库全文中搜索明确关键词（例如在本站搜 RAG 可以找到包含这个词语的文章）</li><li>图书馆系统里搜索书名</li></ul><p>显然词汇搜索有一定的局限性，它<i style="color:tomato;">无法找到同义词、无法理解上下文、拼写错误会影响搜索结果</i>。而 Semantic search（语义搜索）可以补足词汇搜索的这些缺点。</p><h3 id="1-1-2-Semantic-search（语义搜索）"><a href="#1-1-2-Semantic-search（语义搜索）" class="headerlink" title="1.1.2. Semantic search（语义搜索）"></a>1.1.2. Semantic search（语义搜索）</h3><p><span style="color:orange;"><strong>Semantic search（语义搜索）</strong></span>，在当前 RAG 架构主要就是使用 <span style="color:violet;">Vector search（向量搜索）</span>。</p><h4 id="Embedding（文本向量化）"><a href="#Embedding（文本向量化）" class="headerlink" title="Embedding（文本向量化）"></a>Embedding（文本向量化）</h4><p>先把文本转化成向量表示，称为 embedding（文本向量化），向量里包含了语义信息。再计算两个文本的向量距离（相似度），就可以判断它们是否相似。我们只需要知道，把文本用向量来表示是包含语义的，做向量检索就可以匹配到语义相近的文本。各厂商有提供 embedding 模型的接口，直接调用即可。比如调用 <a href="https://platform.openai.com/docs/guides/embeddings/embedding-models">OpenAI - Embeddings</a>，传入一个文本可以得到一个1536维度的数组。</p><p>假设我们的知识库是所有 helpdesk 工单，每个单的文本做 embedding 变成向量存储到数据库里。我们想要搜索和新工单语义最接近的 5 个单，那就把新工单做 embedding 变成一个向量，计算这个向量和数据库里所有向量的距离，相似度最高的 5 个就是我们要找的单。</p><p><img src="/img/2025-07-09-rag-basic/helpdesk_vector_search.png"></p><p>注意，请求搜索的文本做 embedding 时选用的模型，必须与数据库中向量化所使用的模型一致。因为不同模型生成的向量完全不同，不同 embedding 模型之间不能通用。如果要更换 embedding 模型，需要把数据库中已存储的数据重新做 embedding 写入。</p><p>✨ <i style="font-size: 12px;">embedding 模型的好坏会影响检索结果。我认为一般来说已经发布的模型效果不会太差，我会选择当前最新且价格不高的模型。OpenAI 的第三代 embedding 模型 <code>text-embedding-3-large</code> 和 <code>text-embedding-3-small</code> 支持多语言，比之前第二代的 <code>text-embedding-ada-002</code> 只用了英文来训练的效果更好，是个不错的选择。</i></p><p>语义搜索看起来好像比词汇搜索更高级，但也存在不足，它<i style="color:tomato;">不能识别新词和低频词</i>，而这些用词汇搜索是可以实现的。两者互补，使得“词汇搜索+语义搜索”这种既要又要的混合搜索方式成为 RAG 技术栈中的主流。</p><h3 id="1-1-3-Hybrid-search（混合搜索）"><a href="#1-1-3-Hybrid-search（混合搜索）" class="headerlink" title="1.1.3. Hybrid search（混合搜索）"></a>1.1.3. Hybrid search（混合搜索）</h3><blockquote style="padding: 15px; border-radius: 10px; border-left: none; outline: 3px dotted darkslategray;">    <b style="color: orange;">Hybrid search = Lexical search + Semantic search</b>    <ul style="margin: 10px 0; padding-left: 1.5em;">    <li><b>Lexical search</b>（词汇搜索，如关键词搜索、全文搜索、……）    <ul>                <li>优：匹配精准、支持新词/罕见词、能识别拼写精确的查询</li>                <li>劣：缺乏语义理解、无法识别同义词、对拼写错误敏感</li>    </ul>    </li>        <li><b>Semantic search</b>（语义搜索，如向量搜索、……）        <ul>                <li>优：理解上下文含义、支持模糊匹配、能识别表达差异的相似内容</li>                <li>劣：无法识别新词/低频词、对精确查询不敏感、依赖模型一致性</li>        </ul>        </li>    </ul></blockquote><p>✨ <i style="font-size: 12px;">理论上混合搜索是最佳选择，但实际应用中有可能只用词汇搜索或者只用向量搜索就可以达到类似的效果，使用单一的检索策略可以节省资源。使用不同的策略对检索效果的差异取决于具体任务的数据特性和查询需求。我会直接闭眼选择混合搜索。</i></p><h2 id="1-2-Reranking（重排）"><a href="#1-2-Reranking（重排）" class="headerlink" title="1.2. Reranking（重排）"></a>1.2. Reranking（重排）</h2><p>即使使用了混合搜索，但效果可能依然不佳。为了解决词汇搜索和语义搜索的局限性，在进行混合搜索后，又有对检索结果进行二次处理的策略—— reranking。从字面意思看就是重排，在进行初步检索后，训练一个模型来评估请求搜索的文本与每个候选项的匹配度，从而二次排序，得到更准确的结果。</p><p><strong>那做 reranking 和做 embedding 算相似度有什么不同呢？</strong></p><blockquote><p>按我的理解，我会把 <u><b>reranking</b> VS <b>vector search</b></u> 类比成 <u><b>因果推断</b> VS <b>相关性</b></u>。如果你没有统计学的背景，那么我举一个例子说明。</p><p><i style="font-size: 14px;">比如，夏天人们会吃更多的冰淇淋，也更容易晒伤。如果从数据上看，这两件事是正相关。但我们知道吃冰淇淋并不会导致晒伤，晒伤也不会反过来让人去吃冰淇淋。</i></p><p>这就好比 vector search 只能找到数据上算出来是有相关性的内容，而 reranking 进一步判断“这些候选项里，哪些是和用户的问题真正相关的”。</p></blockquote><p><strong>reranking 是如何改善检索效果呢？</strong></p><blockquote>  <p style="font-size: 12px;">（以下为虚拟案例说明 reranking 机制达到的理想效果。）</p>  比如用户查询：<span style="color: Violet;">“法律援助如何申请”</span>  <br>  <br><div style="font-size: 12px;">  <p>初步检索返回的 top 5 结果：</p>  <table border="1" cellspacing="0" cellpadding="4">    <thead>      <tr>        <th>rank</th>        <th>text</th>        <th>remark</th>      </tr>    </thead>    <tbody>      <tr>        <td>1</td>        <td>「中国法律体系包括民法、刑法、行政法等，是国家治理的基石之一。」</td>        <td>（语义有“法律”，但和“申请援助”无关）</td>      </tr>      <tr>        <td>2</td>        <td>「很多人不知道，他们在遇到法律问题时可以申请法律援助，这通常适用于收入较低的人群。」</td>        <td>（提到了“可以申请”，但没说如何申请）</td>      </tr>      <tr>        <td>3</td>        <td>「申请法律援助的第一步是前往当地司法局，填写申请表，并提交收入证明和案件材料。」</td>        <td>✅ （明确回答“如何申请”）</td>      </tr>      <tr>        <td>4</td>        <td>「在一些地区，法律援助可以通过线上平台提交申请，节省了大量时间。」</td>        <td>✅ （也在讲申请流程）</td>      </tr>      <tr>        <td>5</td>        <td>「法律援助制度的发展源于对社会公平正义的追求，其在许多国家已逐步制度化。」</td>        <td>（背景知识，有些远）</td>      </tr>    </tbody>  </table>  <p><br>reranking 后的结果（结合用户问题精准排序）：</p>  <table border="1" cellspacing="0" cellpadding="4">    <thead>      <tr>        <th>rerank</th>        <th>text</th>        <th>remark</th>      </tr>    </thead>    <tbody>      <tr>        <td>1</td>        <td>「申请法律援助的第一步是前往当地司法局，填写申请表，并提交收入证明和案件材料。」</td>        <td>✅ （明确回答“如何申请”）</td>      </tr>      <tr>        <td>2</td>        <td>「在一些地区，法律援助可以通过线上平台提交申请，节省了大量时间。」</td>        <td>✅ （也在讲申请流程）</td>      </tr>      <tr>        <td>3</td>        <td>「很多人不知道，他们在遇到法律问题时可以申请法律援助，这通常适用于收入较低的人群。」</td>        <td>（提到了“可以申请”，但没说如何申请）</td>      </tr>      <tr>        <td>4</td>        <td>「法律援助制度的发展源于对社会公平正义的追求，其在许多国家已逐步制度化。」</td>        <td>（背景知识，有些远）</td>      </tr>      <tr>        <td>5</td>        <td>「中国法律体系包括民法、刑法、行政法等，是国家治理的基石之一。」</td>        <td>（语义有“法律”，但和“申请援助”无关）</td>      </tr>    </tbody>  </table></div></blockquote><p>✨ <i style="font-size: 12px;">reranking 用于优化检索结果，需依赖更精确、计算量更大的模型，因此也会带来响应速度的下降。实际效果因场景而异，有时还可能起反效果。通常 reranking 会在初步检索出的 top 50-100 结果中进行。但如果数据量本就不大，或初步检索后的 top N 中已有不少无关项，引入 reranking 可能反而更累赘。</i></p><h2 id="1-3-交给-LLM-理解的检索结果并不是越多越好"><a href="#1-3-交给-LLM-理解的检索结果并不是越多越好" class="headerlink" title="1.3. 交给 LLM 理解的检索结果并不是越多越好"></a>1.3. 交给 LLM 理解的检索结果并不是越多越好</h2><p>或许你会想，检索结果不够准确，那把足够多的候选项塞给 LLM 去理解生成答案不就完美解决了。LLM 的 input token 能吃下多少就丢多少候选项进去，这样可以尽量减少信息的丢失。然而事实并非如此。</p><p>与 LLM 刚火起来时对比，现在很多 LLM 可以接纳的 input token 变得非常大，让人觉得传给模型的信息越多就能得到越好的效果。从我实际测试的效果来看，传入很长的检索结果（包含一些内容相关较弱的候选项）让 LLM 根据这一大堆内容来回答用户的提问，会降低回复的质量。有论文证实，LLM 在处理长文本时，对开头&#x2F;结尾的信息理解表现比较好，而位于中间的信息更容易被忽略。<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)">[1]</span></a></sup></p><p>所以提高检索结果的准确度，筛选出少量真正相关的候选项，对最终生成的回复非常重要。</p><h2 id="1-4-Vector-database（向量数据库）"><a href="#1-4-Vector-database（向量数据库）" class="headerlink" title="1.4. Vector database（向量数据库）"></a>1.4. Vector database（向量数据库）</h2><p>要实现以上所说的检索，Vector database（向量数据库）就进入了大众视野。现在有很多向量数据库可选，常见的有：Chroma &#x2F; FAISS &#x2F; Qdrant &#x2F; Milvus &#x2F; Pinecone &#x2F; Weaviate &#x2F; Elasticsearch &#x2F; PostgreSQL（+ pgvector插件）等等，还有 Azure 的 AI Search 服务和 Cosmos DB。</p><p>这些向量数据库都支持混合搜索，也可以选择只用词汇搜索或者只用向量搜索。使用向量数据库，无需自己写搜索算法，无需搞清楚两类搜索是怎么混合的，依照各自的文档实现即可。当然在混合搜索里也可以调整一些参数，比如两类搜索的权重，这就需要进一步了解混合搜索实现的细节了。</p><p>✨ <i style="font-size: 12px;">常见的向量数据库都有那么多种选择，可见没有哪一个是完全优于其他的。如何选择适合自己场景的向量数据库，需要基于具体任务对性能、价格和数据安全等多方面进行探索。</i></p><h1 id="2-Chunking（切分）"><a href="#2-Chunking（切分）" class="headerlink" title="2. Chunking（切分）"></a>2. Chunking（切分）</h1><p>Chunking，指的是长文本切分成多个长度较小的 chunk（块）。</p><p><strong>什么情况要做 chunking？</strong></p><blockquote><p>如果要处理的是如 helpdesk 工单、OA 工单、报修工单等这类工单类型的数据，本身文本长度就很短，就没有必要做 chunking。</p><p>如果要处理的是合同、项目报告、会议纪要、审计记录、内部知识库等长文本文档，有可能单个文档就很长，更不用说需要检索出多个候选项合并传给 LLM 了。这时候我们需要把文本内容切分成更小的 chunk，<span style="color:tomato;">一来可以满足 token 限制，二来可以提高检索的准确度</span>。</p></blockquote><p><strong>通用 chunking 策略？</strong></p><blockquote><p>没有标准通用的 chunking 策略。为了满足不同的需求，chunking 策略千差百异。</p><p>文档可以按页切分，按段落切分，按句子切分；还可以按文本长度切分，按 token 长度切分。</p><p>比如从长度上考虑，按页切分就可以解决 token 限制的问题，但如果我要做一个文档对话产品，想要它可以高亮引用句子，那么基于这个需求，就需要按句子切分。</p></blockquote><p><strong>chunking 难点？</strong></p><blockquote><p>如果切分打断了一个语义连贯的段落，会导致检索结果丢失相关信息。</p><p><i style="font-size: 14px;">比如文档内容是这样的：“<span style="color: gray; background-color: #f4d03f;">…… 以下是第一季度房地产市场的整体表现。</span><span style="color: gray; background-color: #58d68d;">该季度一线城市成交量同比增长12%，二线城市价格保持平稳，三线城市则出现轻微回落。……</span>”如果被切成 2 个 chunk。当用户提问：“第一季度房产市场的情况”，检索只会匹配到前一个 chunk，后一个 chunk 会被忽略，而后者才是我们需要的内容。</i></p><p>检索结果丢失了关键信息，那么即使 LLM 厉害到能上天也无法给出正确的答案。</p><p>为了解决 chunking 难题，衍生出五花八门的策略，比如 recursive chunking（初步切分后仍过长或不连贯时，递归寻找更合适的断点），设置前后两个 chunk 之间的文本重叠数量（有些句子或段落同时出现在前后两个 chunk 中），基于语义的切分（通过 embedding 判断语义变化点），等等。</p><p>LLM 领域一个常用的实现工具 LangChain，它的 <a href="https://python.langchain.com/docs/concepts/text_splitters/">Text splitters</a> 模块提供了以上所说的多种切分策略。但实际上并不能完全依赖于开源工具，比如我用高阶的工具识别出表格、图片这些特殊类型的内容，不希望这些内容块被切开，就不能直接使用开源工具，需要自行加工处理。</p></blockquote><h1 id="3-RAG-实现"><a href="#3-RAG-实现" class="headerlink" title="3. RAG 实现"></a>3. RAG 实现</h1><p><img src="/img/2025-07-09-rag-basic/rag_workflow.png" alt="RAG 流程图"><span style="font-size: 12px;">Source: <a href="https://pub.towardsai.net/unlocking-the-advantages-of-semantic-chunking-to-supercharge-your-rag-models-d0daa61bab2c">Unlocking the Advantages of Semantic Chunking to Supercharge Your RAG Models</a></span></p><blockquote style="padding: 15px; border-radius: 10px; border-left: none; outline: 3px dotted darkslategray;">    <b style="color: orange;">RAG 流程</b>    <ul style="margin: 10px 0; padding-left: 1.5em;">    <li><b>数据预处理</b>：chunking -> embedding -> vector database    </li>        <li><b>用户提问</b>：embedding -> retrieve -> generation        </li>    </ul></blockquote><p>RAG 已经是 AI 领域一个相对成熟的架构了，有不少工具可以实现 RAG 的搭建，比如 LangChain &#x2F; LlamaIndex &#x2F; Haystack &#x2F; PydanticAI 等。这些工具可以一定程度上减轻开发 RAG 的工作量，方便对接不同的向量数据库，切换不同的底层 LLM。</p><p>实现 RAG 流程并不难，<span style="color: tomato;">难点在于 chunking 和 retrieve</span>，究竟选择怎样的策略才能使最终回复有更好的效果，这是做每个项目都要具体问题具体验证的。</p><h1 id="〰️✨结语✨〰️"><a href="#〰️✨结语✨〰️" class="headerlink" title="〰️✨结语✨〰️"></a>〰️✨结语✨〰️</h1><p>从本文叙述可以看出，RAG 这个 AI 架构是基于 AI 技术衍生出来的，但其实我们应用 RAG 做产品开发所需要下功夫的大多都不在 AI 上，可以说是源于 AI，又不只是 AI。</p><hr><span style="color: Gray;"><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2307.03172">Lost in the Middle: How Language Models Use Long Contexts</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Tech &amp; Life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI Behind the Scenes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM应用💫不需要学AI，会调接口就行</title>
    <link href="/techlife/llm-application-basic.html"/>
    <url>/techlife/llm-application-basic.html</url>
    
    <content type="html"><![CDATA[<blockquote style="font-size: 14px;color: White; background-color: darkslategray; padding: 15px; border-radius: 10px; outline: 2px dotted darkslategray; outline-offset: 5px;">    <b style="font-size: 16px;">TL;DR</b>    <ul style="margin: 10px 0; padding-left: 1.5em;">        <li>现在发布的 LLM 都是预训练模型，可以通过 API 访问（即是触发 model inference），因此无需知道模型是怎么搭建的，会调接口就可以了。</li>        <li>接口设计 system / user / assistant role 来进行交互。</li>        <li>LLM 没有“记忆”，可以通过拼接历史对话来实现记忆功能。</li>        <li>接口提供的 function calling 功能可以自动触发指定任务，并且返回结构化输出，从而与下游任务交互。</li>        <li>“联网搜索”实际上是做了 2 次 inference，这种通过检索来优化 LLM 生成结果的手段，称为 RAG。</li>    </ul></blockquote><br><div style="text-align: center; font-size: 16px; color: gray;">· · · · · ✦ ✧ ✦ · · · · ·</div><br><p>现在大众说到 AI<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="技术层面的 AI 和大众所说的 AI 有一定的差异。我发现很多民众会认为人脸识别、语音识别这些现在已经是司空见惯的成熟技术了，不是 AI。但其实这些也是 AI。">[1]</span></a></sup>，一般就是指基于 LLM（Large Language Model，大语言模型）搭建的 AI 对话产品。比如最先火起来并且一直在同领域领跑的 ChatGPT（OpenAI），它是一个基于 GPT 系列模型<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="GPT 模型并不是只有一个模型，而是有多个版本的模型，并且还在不断更新迭代。在 ChatGPT 面世之前，OpenAI 已经有 GPT-1 / GPT-2 / GPT-3 这几个版本。初次发行的 ChatGPT 是在 GPT‑3.5 上进行 fine-tune（微调）开发的 AI 对话产品，后续 ChatGPT 采取不同的订阅等级提供给用户不同的模型版本以及其他附加功能。其他 LLM 模型同理，也是系列模型，都在不断迭代中。">[2]</span></a></sup>搭建的智能对话产品。另外几个类似的热门产品有：DeepSeek（DeepSeek AI）、Gemini（Google）、Claude（Anthropic）等。这几个 AI 产品的命名与其底层的系列模型的名称都是一样的。</p><h1 id="1-LLM-飞入寻常百姓家"><a href="#1-LLM-飞入寻常百姓家" class="headerlink" title="1. LLM 飞入寻常百姓家"></a>1. LLM 飞入寻常百姓家</h1><p>LLM 这个概念源自 2017 年 Google 提出的 Transformer 架构。Transformer 的发明使得 NLP（Natural Language Processing，自然语言处理）<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="NLP（Natural Language Processing，自然语言处理）就是研究怎么用计算机去理解人类的语言。简单来说，计算机语言就是写代码，自然语言就是写文章。">[3]</span></a></sup>领域迎来重大突破。而 ChatGPT 的产品化，把 LLM 这项技术转化成了大众可用的工具。开发 ChatGPT 的公司 OpenAI 同时也发布了相关的 API，让开发者可以把 LLM 集成到自己的应用中。后续其他厂商的 LLM 模型发布都采取了 API 的形式，有一些所谓开源的模型还支持本地部署（比如 DeepSeek）。</p><p>造 LLM 这个轮子需要很高的硬件成本和人力成本，但研发的厂商把这些模型以 API 形式发布，对于使用者来说就很友好了。我们不需要考虑模型训练有什么硬件需求，模型训练的参数怎么调，用来训练的数据怎么收集，数据质量高不高，因为发布的这些模型已经训练好了。现在所发布的 LLM 都是<strong>预训练模型（pre-trained model）</strong>。</p><p>我们要使用这些 LLM 的时候就是开袋即食，和食用其他 API 是一样的。想要把 LLM 集成到我们的应用里，搞懂这个 API 怎么 request 和解析 response 就可以了，无需学会 LLM 模型的原理。正如我们使用冰箱不需要知道冰箱是怎么制冷的，知道分冷藏和冷冻就可以了。</p><p>这里以一路领跑的 OpenAI 为例，它奠定了 LLM 以 API 形式发布的基础，现在各大厂商都使用类似的方式发布模型，DeepSeek 甚至是直接设计了 OpenAI 的同款 API（Python SDK），让使用者可以无痛转换。了解了 OpenAI 的接口如何使用，市面上的 LLM 服务都可以适用。</p><h1 id="2-Model-Inference（模型推理）"><a href="#2-Model-Inference（模型推理）" class="headerlink" title="2. Model Inference（模型推理）"></a>2. Model Inference（模型推理）</h1><h2 id="2-1-API-调用"><a href="#2-1-API-调用" class="headerlink" title="2.1 API 调用"></a>2.1 API 调用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ...只展示主要code...</span><br>completion = client.chat.completions.create(<br>    model=<span class="hljs-string">&quot;gpt-4.1&quot;</span>,<br>    messages=[<br>        &#123;<br>            <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>,<br>            <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;Write a one-sentence bedtime story about a unicorn.&quot;</span><br>        &#125;<br>    ]<br>)<br></code></pre></td></tr></table></figure><p>调用 API（<a href="https://platform.openai.com/docs/guides/text?api-mode=chat">OpenAI Chat Completions API</a>），就会触发 LLM 进行 inference（推理）生成文本。</p><p>必填参数2个：</p><ul><li><code>model</code>: 模型版本选择，GPT-3.5 或者 GPT-4 等等，具体模型型号查询：<a href="https://platform.openai.com/docs/models">OpenAI Models list</a></li><li><code>messages</code>:  传给模型的内容，文本内容以不同的 role 传给模型，role 设定见下文。</li></ul><h2 id="2-2-role-设定"><a href="#2-2-role-设定" class="headerlink" title="2.2 role 设定"></a>2.2 role 设定</h2><p>role 有 3 种取值：<font style="color:orange;">system, user, assistant</font></p><h3 id="“user”-用户-“assistant”-AI"><a href="#“user”-用户-“assistant”-AI" class="headerlink" title="“user” - 用户, “assistant” - AI"></a>“user” - 用户, “assistant” - AI</h3><p>以 user &#x2F; assistant 的角色来标记用户和 AI 的一问一答。以 <u>user role</u> 传入文本内容，得到 <u>assistant role</u> 的响应，也就是 LLM 的回复。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-comment">// input</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;介绍一下你自己&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-comment">// output</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;您好！我是由OpenAI开发的人工智能语言模型，名字是ChatGPT。我可以帮助回答各种问题，提供信息、写作建议、学习辅导、创意创作以及与您进行愉快的交流。我的知识截止到2023年10月，所以对那之后的最新事件可能了解有限。如果您有任何问题或需要帮助的地方，请随时告诉我！&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><h3 id="“system”-AI人设"><a href="#“system”-AI人设" class="headerlink" title="“system” - AI人设"></a>“system” - AI人设</h3><p><u>system role</u> 可以看作给 AI 自定义的人设，比如限定背景、身份、语言风格等。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-comment">// input</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;system&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;你叫Hola，是一个资深商业咨询师，你可以为企业解答各种商业问题。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;介绍一下你自己&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-comment">// output</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;您好，我是Hola，一名资深商业咨询师，专注于为企业提供战略规划、运营优化、市场发展、数字转型等方面的专业建议。拥有丰富的行业经验，曾协助多家企业实现业务增长和转型升级。我的目标是帮助企业识别机会、应对挑战，制定切实可行的解决方案，从而实现持续的成功和竞争优势。如有任何商业相关的问题，欢迎随时咨询！&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>可以看到对于用户同样的提问，添加了 system 人设后得到的回复就不一样了。所以集成到应用时可以在后台写好 system role 的定义，控制我们希望用户得到的回复。</p><p>再举个例子，我在调试一个文档对话demo的时候，尝试过不设定 system role，当用户问文档以外的通识性话题时，LLM 会照常给出回复。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-comment">// input</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;介绍一下北京景山公园&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-comment">// output</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;北京景山公园位于北京市中心的故宫博物院西侧，是一座历史悠久、景色优美的城市公园。公园始建于明朝永乐年间（1406年），原为皇家花园，后逐渐发展成为景观公园。景山公园占地面积约68,7公顷，以其巍峨的山丘和丰富的文化底蕴著称。\n公园的核心是景山，它是一座人工堆筑的古城山，高约44.7米，从山顶可以俯瞰整个紫禁城（故宫）和北京城市景观。登上景山顶，可以一览北京的壮丽景色，是游客拍照、欣赏城市全景的绝佳位置。\n除了景山之外，公园内还有许多亭台楼阁、古树名木和湖泊，景色四季各异。春天桃花盛开，夏季绿意盎然，秋天红叶满山，冬天则是银装素裹的景致。\n景山公园不仅是一个休闲娱乐的好去处，也是北京的重要历史文化遗迹之一。它与故宫、天安门广场等地相邻，成为游客体验北京古今交融的必访之地。&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>而在 system role 加上设定限制后，它就不会回复文档以外的内容了。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-comment">// input</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;system&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;你是Smart Reading小助手，请根据用户提问，检索相关文件内容并回答问题。如果检索结果为空，或者检索结果与提问无关，请回答\&quot;抱歉，我无法在文档中找到相关内容。您可以上传相关文件，或者使用支持通用问答的AI助手。\&quot;&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;介绍一下北京景山公园&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-comment">// output</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;抱歉，我无法在文档中找到相关内容。您可以上传相关文件，或者使用支持通用问答的AI助手。&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>在搭建定制化的 AI 对话产品时，需要调的 prompt 主要就是 system role 里的描述。我们如果想要产品更专注于做我们设定范围的服务，在 system role 加限制就很重要。user role 里用户会如何 prompting 是无法限制的。</p><p>如果不是为了搭建 AI 对话，而是要实现某单一功能，设计 system role 和直接只用 user role 没有差别。比如客户评价的情绪识别，读取每条客户评价识别是积极还是消极，那么以下两种实现方式的效果是一样的。显然在这个应用场景里只用 user role 更简洁。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-comment">// input</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;system&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;请识别客户评价的情绪，是积极还是消极&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;&lt;客户评价文本&gt;&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-comment">// input</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;请识别客户评价的情绪，是积极还是消极：&lt;客户评价文本&gt;&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><h2 id="2-3-上下文记忆"><a href="#2-3-上下文记忆" class="headerlink" title="2.3 上下文记忆"></a>2.3 上下文记忆</h2><p>如前面所说，市面上发布的 LLM 都是已经训练好的模型，所以我们做 inference 是不会改变模型的，模型也不会“记住”刚才问过的问题。虽然 LLM 没有“记忆”功能，但我们可以<strong>模拟记忆</strong>来实现多轮对话，把历史对话拼接起来传入 <code>messages</code>。</p><p>第一轮问：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-comment">// input</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;system&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;你叫Hola，是一个资深商业咨询师，你可以为企业解答各种商业问题。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;介绍一下你自己&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>第一轮答：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-comment">// output</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;您好，我是Hola，一名资深商业咨询师，专注于为企业提供战略规划、运营优化、市场发展、数字转型等方面的专业建议。拥有丰富的行业经验，曾协助多家企业实现业务增长和转型升级。我的目标是帮助企业识别机会、应对挑战，制定切实可行的解决方案，从而实现持续的成功和竞争优势。如有任何商业相关的问题，欢迎随时咨询！&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>第二轮问：把第一轮答拼接到第一轮问里，加上用户的新提问，传入 <code>messages</code></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-comment">// input</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;system&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;你叫Hola，是一个资深商业咨询师，你可以为企业解答各种商业问题。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;介绍一下你自己&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;您好，我是Hola，一名资深商业咨询师，专注于为企业提供战略规划、运营优化、市场发展、数字转型等方面的专业建议。拥有丰富的行业经验，曾协助多家企业实现业务增长和转型升级。我的目标是帮助企业识别机会、应对挑战，制定切实可行的解决方案，从而实现持续的成功和竞争优势。如有任何商业相关的问题，欢迎随时咨询！&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;我想要知道房地产行业的最新趋势和挑战&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>第二轮答：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-comment">// output</span><br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;您好，关于房地产行业的最新趋势与挑战，以下是一些关键点供您参考：\n\n一、最新趋势：\n1. 绿色与可持续发展：绿色建筑、环保材料和节能技术日益成为行业标准，推动房地产项目向可持续方向发展。\n2. 数字化转型：引入大数据、物联网、人工智能等技术，实现智慧物业管理、虚拟看房和精准市场分析。\n3. 押注二三线城市和郊区：随着一线城市房价上涨和购房限制放宽，更多投资流向二三线城市和城市郊区，带来新的增长点。\n4. 政策调控与市场深化：政府加强对房地产市场的调控，推动市场健康发展，注重房地产的租赁、共有产权等多样化形式。\n5. 住宅租赁市场崛起：租房市场不断规范，长租公寓和租购并举的模式促进住房多样化。\n\n二、主要挑战：\n1. 政策与调控风险：政府频繁出台限购、限贷等政策，影响市场预期和资金流动。\n2. 资金链压力：土地成本上升、融资环境收紧，对开发商资金链构成压力。\n3. 市场需求变化：随着人口结构变化和年轻一代购房观念转变，传统住宅需求减弱，市场格局调整。\n4. 经济不确定性：宏观经济波动可能影响购房者信心和市场活跃度。\n5. 技术引入与适应：行业需积极适应数字化转型，技术应用的成本和风险也考虑在内。\n\n总结：\n房地产行业正处于深刻变革中，绿色可持续、数字化和市场多元化是未来的主要方向，但政策变化和资金风险也不容忽视。企业可以通过提升技术应用、调整产品策略、关注政策动态等方式，应对行业的挑战，把握发展机遇。\n\n如果您需要更详细的行业分析或针对特定市场的建议，欢迎随时告诉我！&quot;</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>以此类推，以拼接的方式告诉模型之前聊过的内容，就可以实现“记住”历史对话。</p><p>在 ChatGPT &#x2F; DeepSeek 等对话产品界面，点击开启新对话就会“清空记忆”，点击历史对话就可以再续前缘。所以你跟 AI 说“忘掉我们之前说的吧”，其实它会把之前说的历史对话和这一句孟婆汤指令一起传入模型。只有点击开启新对话，才会<strong>真的</strong>忘掉往事。</p><h1 id="3-与下游交互"><a href="#3-与下游交互" class="headerlink" title="3. 与下游交互"></a>3. 与下游交互</h1><p>LLM 所实现的就只是文本生成，这很不智能。如果我们指令让 LLM 生成结构化数据，那不就可以传递给第三方系统进行交互了！</p><p>比如有住户跟物业管家说：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">我是<span class="hljs-selector-tag">B</span>栋<span class="hljs-number">1103</span>的王先生，我家厨房水管爆了，水漏得很厉害，请赶快派人来修！<br></code></pre></td></tr></table></figure><p>我们让 LLM 从中提取相关信息，以 JSON 格式回复：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;resident_name&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;王先生&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;room_number&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;B栋1103&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;repair_type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;水电&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;description&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;厨房水管爆了，水漏得很厉害&quot;</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>然后把这个 JSON 传给物业报修系统去生成一个报修单。</p><h2 id="Function-calling"><a href="#Function-calling" class="headerlink" title="Function calling"></a>Function calling</h2><p>OpenAI 接口的 <a href="https://platform.openai.com/docs/guides/function-calling?api-mode=chat"><strong>Function calling</strong></a> 功能可以实现这个目的。除了 <code>model</code> 和 <code>messages</code> 这两个必填参数，再在 <code>tools</code> 参数里传入以下信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义tool</span><br>repair_order_tool = &#123;<br>    <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;function&quot;</span>,<br>    <span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;create_repair_order&quot;</span>,<br>    <span class="hljs-string">&quot;description&quot;</span>: <span class="hljs-string">&quot;根据住户提供的报修信息生成维修工单&quot;</span>,  <span class="hljs-comment"># tool定义（GPT会根据描述自动识别触发）</span><br>    <span class="hljs-string">&quot;parameters&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;object&quot;</span>,<br>        <span class="hljs-string">&quot;properties&quot;</span>: &#123;     <span class="hljs-comment"># 参数定义（执行调用时可能会传入的参数）</span><br>            <span class="hljs-string">&quot;resident_name&quot;</span>: &#123;<br>                <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;string&quot;</span>, <br>                <span class="hljs-string">&quot;description&quot;</span>: <span class="hljs-string">&quot;住户姓名&quot;</span>   <span class="hljs-comment"># 描述该参数（GPT会根据描述自动提取对应参数）</span><br>            &#125;,  <br>            <span class="hljs-string">&quot;room_number&quot;</span>: &#123;<br>                <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;string&quot;</span>, <br>                <span class="hljs-string">&quot;description&quot;</span>: <span class="hljs-string">&quot;房号，如A栋502&quot;</span><br>            &#125;,<br>            <span class="hljs-string">&quot;repair_type&quot;</span>: &#123;<br>                <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;string&quot;</span>,<br>                <span class="hljs-string">&quot;enum&quot;</span>: [<span class="hljs-string">&quot;水电&quot;</span>, <span class="hljs-string">&quot;家电&quot;</span>, <span class="hljs-string">&quot;门窗&quot;</span>, <span class="hljs-string">&quot;其他&quot;</span>],<br>                <span class="hljs-string">&quot;description&quot;</span>: <span class="hljs-string">&quot;报修问题类型&quot;</span><br>            &#125;,<br>            <span class="hljs-string">&quot;description&quot;</span>: &#123;<br>                <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;string&quot;</span>, <br>                <span class="hljs-string">&quot;description&quot;</span>: <span class="hljs-string">&quot;问题的详细描述&quot;</span><br>            &#125;,  <br>        &#125;,<br>        <span class="hljs-string">&quot;required&quot;</span>: [<span class="hljs-string">&quot;room_number&quot;</span>, <span class="hljs-string">&quot;repair_type&quot;</span>, <span class="hljs-string">&quot;description&quot;</span>]   <span class="hljs-comment"># 定义必须提供的参数</span><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>定义这个 tool 是用来做什么的（”description”: “根据住户提供的报修信息生成维修工单”），LLM 就会根据这个描述来判断是否要触发 function calling。”parameters” 里定义每个参数的含义（每个 parameter 的 description），LLM 就会把对应的参数值抽取出来，得到以上 JSON 格式的结果。</p><p>支持设置多个 tool，LLM 会自动判断应该触发哪个或哪几个 tool。另外一个相关的参数 <code>tool_choice</code>，可以指定必须触发某个 tool。</p><p>不难想象，在 ChatGPT &#x2F; DeepSeek 等对话产品界面，“联网搜索”这个功能就可以通过这种方式来实现。用户打开联网搜索按钮，则指定触发联网搜索这个 tool（第一次 inference），去调用搜索引擎 API，然后把搜索结果传回给 LLM，LLM 就会基于搜索结果生成回复（第二次 inference），用户就可以得到最新信息了。现在 OpenAI 是直接提供 <a href="https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat">Web search</a> 这个功能，不需要通过写 Function calling 来搭建。</p><h1 id="4-与上下游交互"><a href="#4-与上下游交互" class="headerlink" title="4. 与上下游交互"></a>4. 与上下游交互</h1><p>如上文所说，联网搜索功能的实现，在一次对话中其实进行了2次 inference。</p><ul><li><p>第一次 inference：LLM 从用户提问内容中抽取搜索内容，把搜索内容给搜索引擎进行搜索；</p><p>  <img src="/img/2025-06-30-llm-application/bing-openai-gpt-4-1678815799.jpg"></p><p>  从当初 Bing Chat 界面的截图来看，用户提问的是 <i>“does bing use gpt-4?”</i>，而所显示的中间步骤 Searching for 搜索内容是 <i>“bing gpt-4”</i>，可见并不是直接把用户提问直接进行搜索，是用 LLM 识别出用来搜索的关键词再进行搜索的。现在市面上的对话产品都没有显示搜索关键词这个中间步骤了，包括现在又变成叫 Copilot 的 Bing Chat 也没有设计显示搜索关键词。</p></li><li><p>第二次 inference：让 LLM 基于搜索结果回答用户的提问，把 LLM 生成的回复返回给用户。</p>  <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-section"># 取到搜索结果后，传给LLM的prompt</span><br><br>你是一位智能助手。请根据以下搜索结果回答用户的问题，并在回答中使用对应的编号注明信息来源。<br><br>搜索结果：<br><br><span class="hljs-bullet">1.</span> [标题/链接] — 摘要<br><span class="hljs-bullet">2.</span> [标题/链接] — 摘要<br><span class="hljs-bullet">3.</span> [标题/链接] — 摘要<br><br>用户问题：<br>&lt;用户的原始提问&gt;<br><br>回答：<br></code></pre></td></tr></table></figure></li></ul><p>第一次 inference 是 LLM 与下游任务（调用搜索引擎）的交互，第二次 inference 是 LLM 与上游任务（搜索得到结果）的交互。</p><p>这种通过检索来优化 LLM 生成结果的手段，称为 <strong>RAG（Retrieval-Augmented Generation，检索增强生成）</strong>。</p><h2 id="RAG-架构"><a href="#RAG-架构" class="headerlink" title="RAG 架构"></a>RAG 架构</h2><p>RAG 这个概念在 2020 年提出，通过引入外部知识库，解决了预训练模型知识访问受限、难以更新知识、缺乏可解释性等问题。在 ChatGPT 发布后不久，RAG 架构迅速成为热点。现在 RAG 在业界已经是很常见的实现方式了。关于 RAG 的具体原理和实现细节，详见另一篇 blog《<a href="/techlife/rag-concept.html" title="RAG架构💫源于AI，不只AI">RAG架构💫源于AI，不只AI</a>》。</p><h1 id="〰️✨结语✨〰️"><a href="#〰️✨结语✨〰️" class="headerlink" title="〰️✨结语✨〰️"></a>〰️✨结语✨〰️</h1><p>至此，LLM 原生 API 常用的能力基本梳理完毕。归根结底，LLM 只有一种输出，就是文本。<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="此文未讨论多模态的生成模型，仅讨论输入输出都是文本的 LLM。">[4]</span></a></sup>我们看到的各种 RAG、Agent 等高级玩法，都是基于 LLM 生成的文本把各种工具串起来所搭建的。</p><p>大致了解原生 API 的基本能力后，可以把学习重点转向 LangChain &#x2F; LlamaIndex &#x2F; Haystack &#x2F; PydanticAI 等 AI 框架，这些框架已经封装了上下文记忆、历史对话存储、RAG 通用流程等多种常用功能。</p><p>至于从零训练一个 LLM 模型，那是属于少数顶级技术团队才能完成的大工程，我们关注各大厂商的竞逐即可。</p><hr><span style="color: Gray;"><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>技术层面的 AI 和大众所说的 AI 有一定的差异。我发现很多民众会认为人脸识别、语音识别这些现在已经是司空见惯的成熟技术了，不是 AI。但其实这些也是 AI。<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>GPT 模型并不是只有一个模型，而是有多个版本的模型，并且还在不断更新迭代。在 ChatGPT 面世之前，OpenAI 已经有 GPT-1 &#x2F; GPT-2 &#x2F; GPT-3 这几个版本。初次发行的 ChatGPT 是在 GPT‑3.5 上进行 fine-tune（微调）开发的 AI 对话产品，后续 ChatGPT 采取不同的订阅等级提供给用户不同的模型版本以及其他附加功能。其他 LLM 模型同理，也是系列模型，都在不断迭代中。<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>NLP（Natural Language Processing，自然语言处理）就是研究怎么用计算机去理解人类的语言。简单来说，计算机语言就是写代码，自然语言就是写文章。<a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>此文未讨论多模态的生成模型，仅讨论输入输出都是文本的 LLM。<a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Tech &amp; Life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI Behind the Scenes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IT行业的数据“微泄露”</title>
    <link href="/techlife/data-boundary.html"/>
    <url>/techlife/data-boundary.html</url>
    
    <content type="html"><![CDATA[<p>最近迷上了在贝壳上看房源VR，看到一些有趣的信息会跟朋友们分享一下。好友群里有一个素未谋面的同城网友，每次都跳出来表热心，说自己有手段能查到房子有没有问题。当我再次分享房源截图给朋友们看看聊聊，他又跳出来说房源发他查查。我表示并不打算买，只是看到这个房一开始开价过高后面屡次降价，分享出来给大家看看行情，阻止他又开始无脑劝说。结果此举激发了他开启疯狂输出：错，你没懂我在说什么，我说我能查到隐情。😓😓😓😓😓</p><p>此人在体制内IT相关岗位工作，在根本没有人询问的情况下，屡次借话题暗示自己有权限从公安系统查到个人数据。**这种以职务之便侵犯公民个人信息的行为是违法的。**更可耻的是拥有权限的人没有任何保密意识，甚至还热衷于向别人炫耀特权。</p><p>此人被逼急开始在群聊里明确表示自己能查到个人信息，引来了几位群友的关注，询问是否真的可以查到相关信息。很多人会被这样的特权所吸引，希望自己可以从中获利。但其实细想一下，这是一个大大的red flag。他并不觉得暴露他人隐私有什么不妥，不以窥探他人隐私为耻，反而拿来当谈资。这意味着我们群友的信息，也可能随时被他“随手一查然后分享给别人”。真正可怕的不是权限本身，而是掌握权限的人没有职业操守。</p><p>他或许觉得，反正身边人也在这么干，我又没有贩卖信息，这没什么问题啊。这真的没问题吗？</p><p>宁波有一个案件，一位民警应别人要求查询了公民个人信息，将一位女士的住址告知对方，导致这位女士被前男友杀害。</p><blockquote><p>新闻来源：<a href="https://mp.weixin.qq.com/s/7Y70Tv5Yyn_jpc2vlPyDVQ">违规帮忙查住址引发感情命案，民警被判缓刑</a></p></blockquote><p align="center">  <img src="/img/2025-06-18-data-boundary/data_1.jpg" width="40%"></p><p>詹某以权谋私利的时候，或许他想的也是“我只是透露一些信息而已，没什么问题啊”，他也没料到会造成这种严重的后果。</p><p>在互联网时代，只要我们上网就存在信息被泄露的风险。这次我差点被动成为侵害者的同谋，曾几何时我也是受害者。</p><p>几年前刚入职时有一位同事来跟我打招呼，说这里大多数都是我的同龄人，很好相处。对方这句套近乎的话使我内心警铃大作，我的个人信息被泄露了！我所入职的正是公司的数据中心，而这位热心人士与数据组的主管相交甚好，无话不谈。可想而知每位新同事都会被悄悄“起底”，作为他们茶余饭后的谈资。其后我也亲眼目睹了团队里某位数据工程师在看到部门有新同事入职时，熟练地打开SQL查询新同事的个人信息，并且企图与我分享交谈。</p><p>他们对团队成员的八卦有没有影响到当事人，这个不好评判。但是作为曾经的受害者，我是觉得不被尊重的。</p><p>“无心”的炫耀、“顺手”的查询、“无害”的闲聊，这些看似没什么大不了的个人信息泄露，模糊了人对隐私的敬畏。</p><p>你永远无法预知，今天被拿来八卦的那一行信息，明天会不会成为伤害你的工具。</p><p>行业相关的各位同行们，面对你所拥有的数据查询权限，你的底线会是什么呢？</p>]]></content>
    
    
    <categories>
      
      <category>Tech &amp; Life</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>I. 人生终极目标……之一</title>
    <link href="/journey/north-europe-aurora-1.html"/>
    <url>/journey/north-europe-aurora-1.html</url>
    
    <content type="html"><![CDATA[<h1 id="❧-好想好想看看世界另一端的极光"><a href="#❧-好想好想看看世界另一端的极光" class="headerlink" title="❧ 好想好想看看世界另一端的极光"></a>❧ 好想好想看看世界另一端的极光</h1><p>很多人都听过张韶涵的《欧若拉》这首歌，我是从这首歌知道有aurora，中文叫“极光”的东西。但是生活在广东，一个冬天最低气温0°C以上的地区，什么北极圈什么极光对我来说就是世界遥远的另一端。</p><p>后来读书在巴黎住过一段时间，每年圣诞假期都会被北欧极光团的广告吸引。但是真的好贵，上学还花着父母的钱也不好意思做这种娱乐性质的高额消费。眼看着极光团一年比一年贵，看着看着我都结束学业回国了，回到了广州居住，极光就离我更遥远了。</p><p><img src="/img/2025-06-aurora/20250605_01_aurora_dream.jpg" alt="回国了还每年在“想看极光要看极光”，不查回聊天记录都不知道自己意愿这么强烈😆"></p><h1 id="❧-我要闷声做大计"><a href="#❧-我要闷声做大计" class="headerlink" title="❧ 我要闷声做大计"></a>❧ 我要闷声做大计</h1><p>COVID-19阻止了旅行，我每年一游的计划戛然而止。封禁前两年也没什么出门游玩的心情，后面国外基本放开不限制通行了，在网上陆续刷到有人在北欧看到极光大爆发，越来越坐不住了。终于在22年11月底我们也解禁了，出境旅游的苗头熊熊燃起！开始研究独自去极光之旅的可行性。</p><p>没错，从一开始我就打算solo trip。考虑到办签证的难度、对北欧高物价的接受度和要有十天以上的长假期这几个因素，我想不到身边有哪个朋友合适。我不希望这个大project受到别人的干扰，我必须成行！我要闷声做计划！</p><h1 id="❧-雄心壮志中的丝丝犹豫"><a href="#❧-雄心壮志中的丝丝犹豫" class="headerlink" title="❧ 雄心壮志中的丝丝犹豫"></a>❧ 雄心壮志中的丝丝犹豫</h1><p>虽说我立下了雄心壮志要独自前往北欧，但也存在着各种各样的担忧。</p><p>我不是经验丰富的独行者，大部分过去的旅行都是有朋友同行。作为常年生活在亚热带的人，我难以想象在苦寒之地怎么生活。唯一一次遇到大雪纷飞，是多年前圣诞假期跟团去德国玩。好歹德国在我认知中还属于“正常人类居住的地方”，而北欧，在我的概念里就是“发配宁古塔”，我一个南方人往前走分分钟是九死一生！描述是夸张了一点，可见这个旅行在我看来真的是一个巨大的挑战。</p><p>我问了自己两个问题：</p><p><b><span style="color:Red;"><code>Q1</code></span> 北欧旅行花费很高，是不是应该多攒点钱晚几年再去？</b></p><blockquote><p>这个问题我很快就有了答案。别再犹豫，就是现在，当下就去！第一，我的存款是足以支撑这趟旅行的，不至于为了这趟旅行我就要节衣缩食，甚至赊借度日，而且钱花掉了可以再赚。第二，我家里上一辈当前的身体状况不需要我随时守候，我也暂时没有下一辈需要照顾，万一再晚几年，家里对我的需求度很高，我没法出远门，如果我错过了这个本来可以实施的计划，将会成为我心里很大的遗憾。</p></blockquote><p><b><span style="color:Red;"><code>Q2</code></span> 当地的天气状况是超出我的认知范围的，是不是还是找个朋友做伴比较好呢？</b></p><blockquote><p>这个问题其实在我做攻略的时候还会不断出现在我脑海里。一个很客观的事实是，工作之后和朋友是很难对得上时间约在一起的。加上我的同龄人里有一半已婚或者已育，单身的朋友中对旅行有兴趣的也不多，喜欢旅行的朋友大部分对于要办签证出国是有点望而却步的，而且办理申根签证确实是一件麻烦事。这句话说了很多遍，但这里还是要吐槽一句，一生要办签证的中国人啊！总之，做好功课，做足准备，不要让未知的恐惧限制了自己。</p></blockquote><h1 id="❧-怂人的超长前摇——准备时长超过一年"><a href="#❧-怂人的超长前摇——准备时长超过一年" class="headerlink" title="❧ 怂人的超长前摇——准备时长超过一年"></a>❧ 怂人的超长前摇——准备时长超过一年</h1><p>要在冬天长假去北欧，那最可行的日子规划就是春节假期凑上年假。都说极光可遇不可求，我想要尽量待久一点，某种程度上加大几率。</p><p>22年年底放开，23年的春节假期我是没这么快赶上可以立马起行啦，我目标就定在24年春节。23年头两个月我就在关注各种追光团，研究如果要报24年同期的团究竟要提早多久比较好。</p><p>23年在家过了一个没有出门的五一假期，十一假期就坐不住了，新加坡我来啦！此行除了刷新我的旅行地图，是带有其他目的的，就是为我更长途的solo trip预热。</p><p>此前算下来有1.5次的solo trip经验。那0.5次是自己去比利时的首都布鲁塞尔，不过当时是住在法国北部的大城市里尔，在朋友家借宿，从里尔坐火车四十几分钟就可以到布鲁塞尔，当天来回。这个旅程说起来也有段故事，我出行前在巴黎被偷了手机，所以在布鲁塞尔我是没有手机导航，全靠一张简易的纸质地图和我手写的攻略探索这个城市。那1次就是疫情时趁大家都不敢出门头铁去了厦门，电脑都带上了，做好最坏打算被禁足在厦门。也是这次让我发现solo trip是多么的自由自在。</p><p>新加坡之行让我感觉出国旅行还算在我舒适圈里的，磕巴英语在旅行交流是完全够用的。我还复盘了一下我的个人出游行李清单，怎么收拾适合我的使用习惯又节省空间。</p><h1 id="❧-法签曲线救我"><a href="#❧-法签曲线救我" class="headerlink" title="❧ 法签曲线救我"></a>❧ 法签曲线救我</h1><p>就在23年底我正愁年假不够用和挪威签证不好申的时候，出了一则利好消息：法国对拥有法国硕士学位的中国人可发放五年多次的旅行签证。法国签证本来在申根区众多国家中算成功率比较高的了，再加上这个政策，我已经可以预见我的护照上即将贴上五年法签！</p><p>24年的春节前，大概1月底的时候，在TLS网站约递签最早已经只能约到3月1日了。别人说要提前半年准备递签，真的毫不夸张。我准备了学位证书还有成绩单证明自己在法国上过学并且获得了法国的硕士学位，还写了一封动机信深切地表达我的法兰西之情。（此处鸣谢ChatGPT，大大减轻写法语动机信的痛苦，以前留学的时候怎么就没遇上这种神器呢~）</p><p>从递签到出签，仅需3个工作日！这非常不法式，but I like it!</p><p>申根签的使用规则是首次入境需从发签国进入或者是在发签国逗留时间最长。那么如果首次不是从发签国入境，就需要提供在该国逗留时间最长的证明。为了避免这种麻烦，我计划先回一趟法国“激活”我的法签。下一次入境申根直接从北欧四国进入就妥啦！</p><p>然后我就走了一趟法瑞solo trip，十一天的行程，一个30L背包搞定！我对怎么收拾自己的行李已经有自己的最优解决方案了！</p><p>PS：时隔一年了，还是很记得瑞士日内瓦湖的美景有多么叹为观止！</p>]]></content>
    
    
    <categories>
      
      <category>Journey</category>
      
    </categories>
    
    
    <tags>
      
      <tag>我的极光大project</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>那年孔雀没开屏</title>
    <link href="/moments/zoo-memory.html"/>
    <url>/moments/zoo-memory.html</url>
    
    <content type="html"><![CDATA[<p>今天晚上约了在省中医做脚踝MR，没想到现在医院连晚上都能做检查了，确实方便了平时难请假的打工人。不过对医生来说，这恐怕不是什么好事。</p><p>躺在圆滚滚的机器里听了十分钟的噪音就照完了。医生简单说了两句是有炎症。我看他又着急安排下一个病人检查就不好细问，反正会出报告，之后还得约门诊医生复诊。我就去医院门口的公交站坐车回我的小窝。</p><p>上车坐下来一会儿，突然车窗上一个可爱的熊猫贴纸吸引了我的注意。贴纸上写着“大熊猫叫声”，旁边还有个二维码。</p><p><img src="/img/2025-06-04-zoo-memory/20250604_01_bus_zoo.jpg"></p><p>我拿出手机扫码打开了一个视频，画面是两只熊猫照片，声音是咿咿呀呀的熊猫叫声。哇，那一刻真的被可爱到融化。可爱的是熊猫，可爱的是熊猫的叫声，可爱的是这一刻的偶遇。❤️</p><p>我环顾车厢，发现原来这是广州动物园的宣传。车上的小幅横屏在播放着广州动物园的视频。车上贴满了广州动物园的广告。其他车窗也贴了不同小动物的叫声二维码。上车的乘客陆续发现这些小彩蛋，也有人像我一样掏出手机扫码听动物叫声。</p><p><img src="/img/2025-06-04-zoo-memory/20250604_02_bus_zoo.jpg"></p><p>上小红书搜了一下发现，原来我坐上的这趟是广州动物园痛车<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="“痛车”一词来源于日文“痛車（いたしゃ）”，指贴满动漫角色等二次元元素的车子，视觉效果强烈。类似的还有“痛包（痛バッグ）”“痛伞（痛傘）”“痛鞋”等，把喜欢的角色装饰在日常物品上。">[1]</span></a></sup>！一汽巴士和动物园联名的专线。官方宣传视频主打的是广州人的回忆杀，动员大家分享自己和广州动物园的故事。都是老广回忆呀，跟我这个外地人好像没什么关系。</p><p>看完宣传视频放下手机，继续放空欣赏车里的可爱动物。忽然想起来，不对呀，我小时候也是去过广州动物园的！</p><p>在我很小的时候，具体几岁就不记得了，应该还是小学生吧。想不起来我是怎么认识了孔雀这种动物，就吵着要看孔雀。我好想看看孔雀开屏究竟有多美。我们老家是十八线小城市，没有动物园。市中心有一个公园，我记得里面关着几只猴子，没别的动物了。</p><p>那时候我舅已经从老家搬到广州定居，我放假的时候会到广州找舅舅玩。大人们为了满足我的孔雀愿，带我去了广州动物园看孔雀。也不知道当时动物园里是没有孔雀，还是我们没有找到。总之是失望而回了。</p><p>后来我们又去了长隆动物世界，我还是满心期待地找孔雀。结果大人们指着两只秃尾巴的大鸟告诉我，这就是孔雀。我才不懂什么雌孔雀雄孔雀的，这哪里是孔雀啊，别说开屏了，尾巴都是秃的，丑死了。反正呢，我也接受了孔雀开屏是可遇不可求的。看来我从小就习惯躺平。</p><p>当然现在我已经见过孔雀开屏了，绿孔雀见过，白孔雀也见过。我也知道秃尾巴的也是孔雀了。我记不清都在哪里见过的这些孔雀开屏，倒是还记得小时候两次寻找孔雀开屏的失落。</p><p>人生不得意事十常八九。当时当刻的求而不得，或许将来的某一刻会杳然而至呢。当年暑假来广州当游客的小女孩也没想过自己会在广州定居吧。</p><hr><span style="color: Gray;"><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>“痛车”一词来源于日文“痛車（いたしゃ）”，指贴满动漫角色等二次元元素的车子，视觉效果强烈。类似的还有“痛包（痛バッグ）”“痛伞（痛傘）”“痛鞋”等，把喜欢的角色装饰在日常物品上。<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Moments</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Born This Way, Living My Way</title>
    <link href="/moments/lady-gaga-living-my-way.html"/>
    <url>/moments/lady-gaga-living-my-way.html</url>
    
    <content type="html"><![CDATA[<p>The first time I saw Lady Gaga, was in the TV series <em>Gossip Girl</em>. I can’t quite tell the details of that episode. All I can remember is her sharp red dress and the dramatic makeup. It was a MV rehearsal.</p><p>At the time, I thought, “What a weird performance. But the song is fabulous.” Curious about this artist, I looked up her name: <strong>Lady Gaga</strong>. A weird name.</p><p><em>WEIRD</em> - My very first impression about Lady Gaga.</p><p>But somehow, that weirdness caught my attention.</p><p><em>Bad Romance</em>, the song performed in the TV series, the first Lady Gaga track I ever heard, and the one that made me a fan, a Little Monster.</p><p>She went viral in the pop music world. I followed everything about her—albums, music videos, live shows, interviews.</p><p>I like her music, which tells the spirit of strong, brave and confidence. Her performance has an intense vibe, but she’s so gentle and calm in interviews. She’s quite different on and off stage.</p><p>I had watched all her shows online but never considered seeing an American pop star perform live, until I saw her tour poster at Zénith de Paris, the same arena where I had just attended Eason Chan’s concert. It’s a pity that I didn’t have any friends interested in her music, and at that time, I didn’t have the courage to attend a concert alone.</p><p>Last year, while planning my travels around the world, I made up my mind that I had to see her live.</p><p>This year, her Singapore show is right within my reach. Over the past few years, I’ve traveled the world alone and have visited Singapore twice, becoming somewhat familiar with the city. I feel ready and energized to attend her concert by myself this time. However, I’ve decided to prioritize saving for an apartment instead. So, I gave up on the idea from the very start.</p><p>I regret missing the chance 11 years ago, but this time, I feel no disappointment. I know exactly what I want to do. To me, Lady Gaga’s spirit is all about embracing who we truly are and living life on our own terms. If I were to meet her in person and tell her about my decision, I believe she would appreciate it.</p>]]></content>
    
    
    <categories>
      
      <category>Moments</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>我既是母亲，也是我自己</title>
    <link href="/moments/mothers-day.html"/>
    <url>/moments/mothers-day.html</url>
    
    <content type="html"><![CDATA[<p>今年母亲节看到一个老同学发的朋友圈，配文*『我们不止是孩子的妈妈，也要有自己的精彩人生🔅ᵕ̈ 祝你我快乐』*。</p><p>我发现我身边同龄或者稍微年长的“妈妈们”活得各有各的精彩。有的妈妈挤时间提升专业知识，没有因为孩子停止专业追求。有的妈妈为了方便带娃阶段性转换工作，没有因为孩子放弃职场。有的妈妈不爱做饭就给孩子点外卖，没有因为孩子没苦硬吃。有的妈妈工作认真负责，没有因为孩子多病要经常请假就推脱工作。</p><p>当这一代人成为母亲，越来越多的女性会有这样的意识，母亲只是我众多身份当中的一个。<strong>我既是母亲，也是我自己。</strong></p><p>在网上看到一段触动人心的小故事。网友的妈妈在母亲节说感谢女儿，因为女儿所以她有了母亲这个身份，觉得自己成为母亲是很骄傲的事。</p><p>成为母亲，是我的选择，而不是迫于社会压力。我不需要通过宣扬母亲的苦难来抵消被迫生儿育女的伤害。</p><p>愿生活在这个时代的我和我的女性朋友们，成为母亲是自己的伟大选择。</p>]]></content>
    
    
    <categories>
      
      <category>Moments</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>好东西，就这？</title>
    <link href="/feed/her-story.html"/>
    <url>/feed/her-story.html</url>
    
    <content type="html"><![CDATA[<p>五一放假在家终于看了之前大热的电影《好东西》。</p><p>两位女主非常养眼，视觉效果相当的赏心悦目。钟楚曦饰演的小叶，乐队主唱，一个寻爱寻到海王的单身姑娘，为了和对方保持联系，谎称自己已婚有孩假装潇洒。宋佳饰演的王铁梅，前记者现任自媒体编辑，离异带女儿，和小叶做了邻居，前夫经常来纠缠复合，和新认识的年轻弟弟保持肉体关系。叙事风格平淡，剧情很平实没有什么波澜，就好像是听了一个没什么八卦价值的邻里小事。</p><p>不过当中的一些小情节有引发到我的共鸣。<strong>什么是好东西？能让你开心的就是好东西。</strong></p><p>我爸老喜欢质问我，你做这个东西有什么意义？打游戏有什么意义？看剧有什么意义？怎么就没意义了，能愉悦我，这就是它的意义！</p><p>之前在社交媒体上屡次刷到这部电影的好评，观影者都给它加上了时下正盛行的“女性主义”标签。女性主义在当今中国的社交媒体是一个相当热门的社会议题。</p><p>什么是女性主义，如何定义女性主义，我暂时还没找到一个明确的答案。我能肯定的是，《好东西》这部电影，和<strong>女性主义</strong>毫无关系。电影只是平铺直述了两位女性的困境，没有展开解读，也没有总结拔高，找不到任何触动点。我会把它归类为<strong>中年疼痛文学</strong>。把它和女性主义扯上关系，甚至打上了女性主义代表作的标签，就和电影里前夫哥生搬女性主义词汇一样令人尴尬到脚趾抠地。</p><p>不知道电影宣传的时候有没有打着女性主义的旗号，还是后期被媒体冠上的虚名，反正在我观影过程中从头到尾都没有发现导演有表达女性主义这个意图。</p><p>我很困惑当我们谈到女性成长、女性觉醒、女性主义这些话题，大家很喜欢塑造“离开男人也能很精彩”的独立女性形象，比如电影里两位女主的人设。刻意牵扯男性而言的“独立”，我总觉得哪里不对劲。</p>]]></content>
    
    
    <categories>
      
      <category>Feed</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
